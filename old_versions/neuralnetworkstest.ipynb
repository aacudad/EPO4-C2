{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import butter, iirnotch, lfilter\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "from ecgdetectors import Detectors\n",
    "import pyhrv.frequency_domain as fd\n",
    "import pyhrv.time_domain as td\n",
    "import pyhrv\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reads a csv file and outputs a data frame\n",
    "def read_from_csv(file):\n",
    "    data_frame = pd.read_csv(file, sep=\",\")\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "[1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "df_features = read_from_csv('out_features.csv')\n",
    "df_features.head()\n",
    "y = []\n",
    "for i in range(15):\n",
    "    y.extend([1,2,1,1])\n",
    "\n",
    "print(len(y))\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  feat_nn20  feat_nn50  feat_rmssd   feat_nnhr   feat_sdsd  \\\n",
      "22        S73      275.0      163.0    0.076788   58.710806   78.965915   \n",
      "33       S102      273.0       64.0    0.063904  108.861348   35.787715   \n",
      "15        S54      369.0       84.0    0.034627   64.224175   32.497720   \n",
      "29        S92      481.0      174.0    0.084124   80.729926   63.333814   \n",
      "18        S63      250.0       75.0    0.046496   66.343922   42.363071   \n",
      "41       S132      234.0       43.0    0.037942  109.772265   20.998153   \n",
      "27        S84      683.0      372.0    0.089754   70.722119   77.596404   \n",
      "31        S94      484.0      135.0    0.055477   72.767872   46.114963   \n",
      "57       S172      309.0      111.0    0.073406  115.732132   38.850940   \n",
      "13        S52      189.0       12.0    0.095763   90.134331   64.318413   \n",
      "10        S43      288.0      171.0    0.082060   62.428316   79.565409   \n",
      "19        S64      600.0      247.0    0.064856   67.494824   58.261392   \n",
      "34       S103      267.0       82.0    0.057654   76.736619   45.491110   \n",
      "59       S174      595.0      393.0    0.125245   63.686270  121.344928   \n",
      "53       S162       42.0        0.0    0.015776  129.152944    7.390225   \n",
      "48       S151      570.0       92.0    0.041522   83.240080   30.113308   \n",
      "52       S161      967.0      513.0    0.067642   65.949997   62.194415   \n",
      "47       S144      442.0      102.0    0.040973   77.387464   31.926011   \n",
      "2         S23      290.0      160.0    0.151828   66.419476  138.993950   \n",
      "51       S154      593.0      274.0    0.077041   70.129566   66.658842   \n",
      "49       S152      547.0      174.0    0.059727   85.458486   42.286841   \n",
      "21        S72      560.0      356.0    0.150606   87.271041  107.538716   \n",
      "0         S21     1023.0      513.0    0.087298   73.562499   72.089815   \n",
      "3         S24      632.0      354.0    0.167015   67.048509  151.520129   \n",
      "42       S133      211.0       59.0    0.061579   87.194769   42.749034   \n",
      "38       S113      318.0      144.0    0.082969   75.284261   67.307052   \n",
      "44       S141      749.0      191.0    0.040322   73.577746   33.046345   \n",
      "36       S111     1056.0      443.0    0.060438   74.597417   48.924786   \n",
      "54       S163      180.0       24.0    0.035874   86.115138   25.178470   \n",
      "30        S93       67.0        6.0    0.021998   86.068997   15.393590   \n",
      "45       S142       66.0       10.0    0.043924  127.739788   20.834607   \n",
      "39       S114      661.0      260.0    0.071704   86.334632   50.856654   \n",
      "9         S42      528.0      181.0    0.056685   80.609446   42.721449   \n",
      "50       S153      223.0       66.0    0.062286   77.163256   48.805782   \n",
      "8         S41      933.0      647.0    0.091420   61.212739   90.717528   \n",
      "23        S74      639.0      400.0    0.121007   61.931864  120.214761   \n",
      "40       S131      481.0      104.0    0.046864   87.461828   32.340897   \n",
      "1         S22      405.0      149.0    0.165925   77.457380  131.037999   \n",
      "5         S32      477.0      206.0    0.085018   89.796388   58.936560   \n",
      "55       S164      634.0      311.0    0.085311   76.561397   68.276058   \n",
      "46       S143      149.0       19.0    0.029837   83.582643   21.527400   \n",
      "58       S173      309.0      128.0    0.064518   76.203624   51.830145   \n",
      "\n",
      "    feat_pnn50  feat_pnn20  peak vlf   peak lf   peak hf  norm power lf  \\\n",
      "22    0.452778    0.763889  0.015625  0.091797  0.173828      29.808534   \n",
      "33    0.049536    0.211300  0.013672  0.040039  0.162109      55.570839   \n",
      "15    0.099526    0.437204  0.014648  0.113281  0.151367      71.754435   \n",
      "29    0.203509    0.562573  0.016602  0.149414  0.155273      42.507841   \n",
      "18    0.184275    0.614251  0.018555  0.041016  0.163086      74.267458   \n",
      "41    0.035893    0.195326  0.011719  0.137695  0.163086      61.112589   \n",
      "27    0.404788    0.743199  0.039062  0.132812  0.150391      56.304981   \n",
      "31    0.141807    0.508403  0.014648  0.118164  0.164062      55.794843   \n",
      "57    0.081378    0.226540  0.013672  0.040039  0.184570      42.593525   \n",
      "13    0.012513    0.197080  0.013672  0.122070  0.234375      48.934261   \n",
      "10    0.447644    0.753927  0.015625  0.097656  0.240234      55.591711   \n",
      "19    0.282609    0.686499  0.016602  0.145508  0.150391      60.325862   \n",
      "34    0.174840    0.569296  0.001953  0.136719  0.155273      55.238087   \n",
      "59    0.521912    0.790173  0.017578  0.149414  0.157227      29.020948   \n",
      "53    0.000000    0.029289  0.011719  0.128906  0.166016      75.083374   \n",
      "48    0.056860    0.352287  0.021484  0.083984  0.150391      73.443643   \n",
      "52    0.400468    0.754879  0.028320  0.040039  0.150391      58.475188   \n",
      "47    0.100196    0.434185  0.015625  0.123047  0.161133      58.691591   \n",
      "2     0.406091    0.736041  0.024414  0.126953  0.163086      60.448215   \n",
      "51    0.299127    0.647380  0.018555  0.040039  0.170898      29.015023   \n",
      "49    0.179938    0.565667  0.016602  0.040039  0.165039      44.944487   \n",
      "21    0.397765    0.625698  0.012695  0.149414  0.242188      11.394000   \n",
      "0     0.370665    0.739162  0.016602  0.149414  0.151367      52.982179   \n",
      "3     0.418935    0.747929  0.037109  0.040039  0.162109      36.163556   \n",
      "42    0.107664    0.385036  0.004883  0.128906  0.190430      65.618729   \n",
      "38    0.318584    0.703540  0.004883  0.149414  0.151367      61.787599   \n",
      "44    0.132823    0.520862  0.018555  0.040039  0.150391      79.873200   \n",
      "36    0.304258    0.725275  0.018555  0.040039  0.150391      47.725385   \n",
      "54    0.045889    0.344168  0.002930  0.141602  0.162109      75.308552   \n",
      "30    0.011321    0.126415  0.003906  0.115234  0.162109      86.323657   \n",
      "45    0.007037    0.046446  0.011719  0.111328  0.188477      58.664263   \n",
      "39    0.233603    0.593890  0.030273  0.130859  0.153320      67.421321   \n",
      "9     0.215220    0.627824  0.013672  0.040039  0.150391      48.333381   \n",
      "50    0.139535    0.471459  0.012695  0.111328  0.167969      55.013428   \n",
      "8     0.554889    0.800172  0.013672  0.052734  0.166992      37.998777   \n",
      "23    0.504414    0.805801  0.014648  0.121094  0.150391      59.243504   \n",
      "40    0.060890    0.281616  0.029297  0.040039  0.150391      52.648286   \n",
      "1     0.191763    0.521236  0.024414  0.040039  0.152344      30.783575   \n",
      "5     0.223670    0.517915  0.013672  0.040039  0.171875      48.274493   \n",
      "55    0.314777    0.641700  0.015625  0.138672  0.150391      53.249383   \n",
      "46    0.036965    0.289883  0.023438  0.125000  0.165039      75.897866   \n",
      "58    0.277056    0.668831  0.032227  0.042969  0.182617      53.418548   \n",
      "\n",
      "    norm power hf  power ratio  total power  \n",
      "22      70.191466     0.424675  2084.075154  \n",
      "33      44.429161     1.250774  1887.361131  \n",
      "15      28.245565     2.540379  1520.890152  \n",
      "29      57.492159     0.739368  3246.457109  \n",
      "18      25.732542     2.886130  2757.607494  \n",
      "41      38.887411     1.571526  1110.553973  \n",
      "27      43.695019     1.288590  6350.424268  \n",
      "31      44.205157     1.262179  2514.422947  \n",
      "57      57.406475     0.741964  2149.720743  \n",
      "13      51.065739     0.958260  2144.427386  \n",
      "10      44.408289     1.251832  2606.392691  \n",
      "19      39.674138     1.520534  3687.125331  \n",
      "34      44.761913     1.234042  2255.455306  \n",
      "59      70.979052     0.408866  9117.220263  \n",
      "53      24.916626     3.013384   281.668544  \n",
      "48      26.556357     2.765577  1524.959363  \n",
      "52      41.524812     1.408199  3311.543645  \n",
      "47      41.308409     1.420815  1318.676646  \n",
      "2       39.551785     1.528331  3156.276606  \n",
      "51      70.984977     0.408749  3533.399349  \n",
      "49      55.055513     0.816349  1650.134787  \n",
      "21      88.606000     0.128592  6358.347156  \n",
      "0       47.017821     1.126853  1605.136328  \n",
      "3       63.836444     0.566503  6978.411314  \n",
      "42      34.381271     1.908560  2281.995459  \n",
      "38      38.212401     1.616951  5138.499085  \n",
      "44      20.126800     3.968500  1113.243901  \n",
      "36      52.274615     0.912974  1468.522632  \n",
      "54      24.691448     3.049985  1063.175391  \n",
      "30      13.676343     6.311896   704.446843  \n",
      "45      41.335737     1.419214   663.510875  \n",
      "39      32.578679     2.069492  5187.113482  \n",
      "9       51.666619     0.935486  2455.758117  \n",
      "50      44.986572     1.222886  2256.008796  \n",
      "8       62.001223     0.612871  3652.702473  \n",
      "23      40.756496     1.453597  9840.817455  \n",
      "40      47.351714     1.111856  1470.973259  \n",
      "1       69.216425     0.444744  5017.042622  \n",
      "5       51.725507     0.933282  5004.680765  \n",
      "55      46.750617     1.139009  5379.765404  \n",
      "46      24.102134     3.149010  1091.793890  \n",
      "58      46.581452     1.146777  4406.044112  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'S73'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(X_train)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Scale the data\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\u001b[39m.\u001b[39;49mfit(X_train)\n\u001b[0;32m     13\u001b[0m X_train \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_train)\n\u001b[0;32m     14\u001b[0m X_test \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_test)\n",
      "File \u001b[1;32mc:\\Users\\riche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 824\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\riche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    860\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 861\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    862\u001b[0m     X,\n\u001b[0;32m    863\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    864\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    865\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    866\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[0;32m    867\u001b[0m )\n\u001b[0;32m    868\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    870\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\riche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\riche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\riche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\riche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'S73'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=14)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Define the feedforward neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(14,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_pca, y_train, validation_data=(X_test_pca, y_test), epochs=75, batch_size=100, verbose=2)\n",
    "val_accuracies = []\n",
    "for i in range(10):\n",
    "    # ... your code ...\n",
    "    history = model.fit(X_train_pca, y_train, validation_data=(X_test_pca, y_test), epochs=10, batch_size=32, verbose=2)\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "avg_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n",
    "print(\"Average Validation Accuracy:\", avg_val_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
