{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import neurokit2 as nk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pyhrv.time_domain as td\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pyhrv\n",
    "import ipynb\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# functions for features extraction\n",
    "from ipynb.fs.full.ECG_features import get_ecgfeatures\n",
    "from ipynb.fs.full.EDA import get_edaindex, get_edafeatures\n",
    "from ipynb.fs.full.BVP import get_bvpfeatures, bvp_prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection & Feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset reading\n",
    "class read_data_of_one_subject:\n",
    "            \"\"\"Read data from WESAD dataset\"\"\"\n",
    "            def __init__(self, path, subject):\n",
    "                self.keys = ['label', 'subject', 'signal']\n",
    "                self.signal_keys = ['wrist', 'chest']\n",
    "                self.chest_sensor_keys = ['ACC', 'ECG', 'EDA', 'EMG', 'Resp', 'Temp']\n",
    "                self.wrist_sensor_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "                #os.chdir(path)\n",
    "                #os.chdir(subject)\n",
    "                with open(path + subject +'/'+subject + '.pkl', 'rb') as file:\n",
    "                    data = pickle.load(file, encoding='latin1')\n",
    "                self.data = data\n",
    "\n",
    "            def get_labels(self):\n",
    "                return self.data[self.keys[0]]\n",
    "\n",
    "            def get_wrist_data(self):\n",
    "                \"\"\"\"\"\"\n",
    "                #label = self.data[self.keys[0]]\n",
    "                #assert subject == self.data[self.keys[1]]\n",
    "                signal = self.data[self.keys[2]]\n",
    "                wrist_data = signal[self.signal_keys[0]]\n",
    "                #wrist_ACC = wrist_data[self.wrist_sensor_keys[0]]\n",
    "                #wrist_ECG = wrist_data[self.wrist_sensor_keys[1]]\n",
    "                return wrist_data\n",
    "\n",
    "            def get_chest_data(self):\n",
    "                \"\"\"\"\"\"\n",
    "                signal = self.data[self.keys[2]]\n",
    "                chest_data = signal[self.signal_keys[1]]\n",
    "                return chest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "subject: S2\n",
      "total samples S2: 29\n",
      "subject: S3\n",
      "total samples S3: 29\n",
      "subject: S4\n",
      "total samples S4: 29\n",
      "subject: S5\n",
      "total samples S5: 29\n",
      "subject: S6\n",
      "total samples S6: 29\n",
      "subject: S7\n",
      "total samples S7: 29\n",
      "subject: S8\n",
      "total samples S8: 29\n",
      "subject: S9\n",
      "total samples S9: 29\n",
      "subject: S10\n",
      "total samples S10: 30\n",
      "subject: S11\n",
      "total samples S11: 30\n",
      "subject: S12\n",
      "total samples S12: 0\n",
      "subject: S13\n",
      "total samples S13: 29\n",
      "subject: S14\n",
      "total samples S14: 30\n",
      "subject: S15\n",
      "total samples S15: 30\n",
      "subject: S16\n",
      "total samples S16: 29\n",
      "subject: S17\n",
      "total samples S17: 29\n"
     ]
    }
   ],
   "source": [
    "# Set up empty dataframes for the features\n",
    "columns_ecg =['index', 'ecg_HR_mean', 'ecg_HR_min', 'ecg_HR_max', 'ecg_HR_std', 'ecg_SDNN', 'ecg_SDANN', 'ecg_RMSSD', \n",
    "                                           'ecg_SDSD','ecg_pNN50', 'ecg_pNN20', \"ecg_triangular_index\", \"ecg_tinn\", \"ecg_sd1\", \"ecg_sd2\",\n",
    "                                            \"ecg_ratio_sd2_sd1\", 'ecg_abs_power_VLF', 'ecg_abs_power_LF', 'ecg_abs_power_HF', 'ecg_tot_power',\n",
    "                                            'ecg_LF/HF', 'ecg_peak_vlf', 'ecg_peak_lf', 'ecg_peak_hf', 'ecg_norm_power_LF', 'ecg_norm_power_HF',\n",
    "                                            'max_cwt_absmean', 'min_cwt_absmean', 'mean_cwt_absmean','std_cwt_absmean', \n",
    "                                            'max_cwt_std', 'min_cwt_std', 'mean_cwt_std','std_cwt_std',\n",
    "                                            'max_cwt_energy', 'min_cwt_energy', 'mean_cwt_energy','std_cwt_energy',\n",
    "                                            'max_cwt_var', 'min_cwt_var', 'mean_cwt_var','std_cwt_var',\n",
    "                                            'entropy']\n",
    "columns_resp = ['RSP_Rate_Mean','RRV_RMSSD', 'RRV_MeanBB', 'RRV_SDBB', 'RRV_SDSD', 'RRV_CVBB', 'RRV_CVSD', 'RRV_MedianBB',\n",
    "                'RRV_MadBB', 'RRV_MCVBB', 'RRV_VLF', 'RRV_LF', 'RRV_HF', 'RRV_LFHF', 'RRV_LFn', 'RRV_HFn', 'RRV_SD1',\n",
    "                'RRV_SD2', 'RRV_SD2SD1', 'RRV_ApEn', 'RRV_SampEn', 'RSP_Amplitude_Mean', 'RSP_RVT', 'RSP_Symmetry_PeakTrough',\n",
    "                'RSP_Symmetry_RiseDecay', 'RSP_Phase_Duration_Inspiration', 'RSP_Phase_Duration_Expiration', 'RSP_Phase_Duration_Ratio']\n",
    "\n",
    "columns = [*columns_ecg, *columns_resp]\n",
    "ecgfeatures = pd.DataFrame(None, columns= columns)\n",
    "edafeatures = None\n",
    "bvpfeatures = None\n",
    "\n",
    "print(len(ecgfeatures.keys()))\n",
    "total_samples = 0\n",
    "fs = 700 # sampling frequency\n",
    "fs_bvp = 64\n",
    "duration = 45\n",
    "y = []\n",
    "amountpsample = dict()\n",
    "# data_set_path= \"C:/Users/JackC/Documents/EPO4/WESAD/WESAD/\" # Folder path\n",
    "data_set_path= \"C:/Users/riche/Downloads/WESAD/WESAD/\" # Folder path\n",
    "# data_set_path = \"C:/Users/Adnane/Downloads/WESAD/WESAD/\"\n",
    "\n",
    "for i in range(16): # 15 subjects\n",
    "    subject = 'S'+str(i+2) # Cycle through S2 to S17\n",
    "    print(f\"subject: {subject}\")\n",
    "    amountpsample[subject] = 0\n",
    "    if subject != 'S12': # Skip S12, because it does not exist\n",
    "        # Object instantiation\n",
    "        obj_data = {}\n",
    "\n",
    "        # Accessing class attributes and method through objects\n",
    "        obj_data[subject] = read_data_of_one_subject(data_set_path, subject)\n",
    "\n",
    "        chest_data_dict = obj_data[subject].get_chest_data()\n",
    "        wrist_data_dict = obj_data[subject].get_wrist_data()\n",
    "        chest_dict_length = {key: len(value) for key, value in chest_data_dict.items()}\n",
    "        wrist_dict_length = {key: len(value) for key, value in wrist_data_dict.items()}\n",
    "\n",
    "        # Get labels\n",
    "        labels = obj_data[subject].get_labels()\n",
    "\n",
    "        for label in [1,2,4]: # for every state of stress\n",
    "            \n",
    "            baseline = np.asarray([idx for idx,val in enumerate(labels) if val == label])\n",
    "\n",
    "            # Obtaining the chest data\n",
    "            eda_base=chest_data_dict['EDA'][baseline,0] # Select the EDA data\n",
    "            ecg_base=chest_data_dict['ECG'][baseline,0] # Select the ECG data\n",
    "            resp_base=chest_data_dict['Resp'][baseline,0] # Select the respiration data\n",
    "\n",
    "            # Obtaining the wrist data\n",
    "            baseline_BVP = baseline * fs_bvp // fs\n",
    "            bvp_base=wrist_data_dict['BVP'][baseline_BVP,0] # Select the BVP data\n",
    "\n",
    "            # Multiple smaller samples per signal\n",
    "            # samples = 10 # specify amount of samples\n",
    "\n",
    "            for j in range(0, len(eda_base)//(duration*fs), 2): # loop over all segments\n",
    "                amountpsample[subject] += 1\n",
    "                # adding the labels to y\n",
    "                if label==1 or label==4: # if amusement or medidation, classify as non-stress\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                # Cutting the signal into a segment\n",
    "                start, end = j*fs*duration, (j+1)*fs*duration\n",
    "                ecg = ecg_base[start: end]\n",
    "                eda = eda_base[start: end]\n",
    "                bvp = bvp_base[start: end]\n",
    "\n",
    "                # splitting sample into 10 smaller even-sized parts\n",
    "                #ecg = ecg_base[int(len(ecg_base)* (j/samples)) : int(len(ecg_base)* ((j+1)/samples))]\n",
    "                #eda = eda_base[int(len(eda_base)* (j/samples)) : int(len(eda_base)* ((j+1)/samples))]\n",
    "\n",
    "                # getting the eda and ecg features\n",
    "                index = subject + str(label) + str(j)\n",
    "                tonic, phasic, start, end = get_edaindex(eda, fs)\n",
    "                edafeatures = get_edafeatures(index, edafeatures, phasic, tonic, fs)\n",
    "                ecgfeatures = get_ecgfeatures(ecg, fs, ecgfeatures, index)\n",
    "                \n",
    "                bvpfeature = get_bvpfeatures(bvp, fs_bvp, index)\n",
    "                bvpfeatures = pd.concat([bvpfeatures, bvpfeature], axis=0)\n",
    "    print(f\"total samples {subject}: {amountpsample[subject]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_edafeatures = edafeatures\n",
    "# saved_ecgfeatures = ecgfeatures\n",
    "# saved_bvpfeatures = bvpfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riche\\OneDrive\\Documenten\\GitHub\\EPO4-C2\\BVP.ipynb:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"    features_event.reset_index(drop=True, inplace=True)\\n\",\n"
     ]
    }
   ],
   "source": [
    "bvp_filt = bvp_prep(bvpfeatures) # remove nan and inf in BVP features\n",
    "# ecgfeatures = bvp_prep(ecgfeatures)\n",
    "\n",
    "# saving the feature data in csv files\n",
    "edafeatures.to_csv(\"EDA_features.csv\")\n",
    "ecgfeatures.to_csv(\"ECG_features.csv\")\n",
    "bvp_filt.to_csv(\"BVP_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #reading from csv file if necessary\n",
    "# edafeatures = pd.read_csv(\"EDA_features.csv\", index_col=0)\n",
    "# ecgfeatures = pd.read_csv(\"ECG_features.csv\", index_col=0)\n",
    "# bvp_filt = pd.read_csv(\"BVP_features.csv\", index_col=0)\n",
    "#features = pd.read_csv(\"features_\"+str(samples)+\"_samp.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = pd.merge(ecgfeatures, edafeatures, left_index=True, right_index=True)\n",
    "total_features = pd.merge(bvp_filt, features1, on='index')\n",
    "total_features['y'] = y\n",
    "\n",
    "total_features = total_features[total_features.RSP_RVT != 0.0]\n",
    "total_features.to_csv(\"out_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_total_features = total_features\n",
    "# total_features = pd.read_csv(\"out_features.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riche\\OneDrive\\Documenten\\GitHub\\EPO4-C2\\BVP.ipynb:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"    features_event.reset_index(drop=True, inplace=True)\\n\",\n"
     ]
    }
   ],
   "source": [
    "total_features = bvp_prep(total_features)\n",
    "features = total_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402 402\n"
     ]
    }
   ],
   "source": [
    "# Scaling the data\n",
    "# scaler = StandardScaler().fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "num_features = 25\n",
    "y = total_features['y']\n",
    "X_total = total_features.drop(['y', 'index'], axis=1)\n",
    "print(len(y), len(X_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Variance Threshold\n",
    "# scaler = MinMaxScaler().fit(X_total)\n",
    "# X_train = scaler.transform(X_total)\n",
    "\n",
    "# total_features_index = []\n",
    "# for feat in X_total:\n",
    "#     total_features_index.append(feat)\n",
    "\n",
    "# X_vt = X_train[0:,0:len(X_total.keys())]\n",
    "\n",
    "# v_threshold = VarianceThreshold(threshold=0.005) # Set a threshold\n",
    "# v_threshold.fit(X_vt)\n",
    "# index = v_threshold.get_support()\n",
    "# true_index = [i for i, x in enumerate(index) if x]\n",
    "# vt_features = [total_features_index[i] for i in true_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pearson correlation\n",
    "def cor_selector(X_cor, y,num_feats):\n",
    "    cor_list = []\n",
    "    feature_name = X_cor.columns.tolist()\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X_cor.columns.tolist():\n",
    "        cor = np.corrcoef(X_cor[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X_cor.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    return cor_support, cor_feature\n",
    "\n",
    "def pears(X):\n",
    "    cor_support, cor_feature = cor_selector(X, y, num_features)\n",
    "    return cor_feature\n",
    "\n",
    "pc_features = pears(X_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi-squared\n",
    "X_norm = MinMaxScaler().fit_transform(X_total)\n",
    "\n",
    "chi_selector = SelectKBest(chi2, k=num_features)\n",
    "chi_selector.fit(X_norm, y)\n",
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X_total.loc[:,chi_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper Forwards\n",
    "sfs = SFS(svm.SVC(),\n",
    "          k_features=num_features,\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring = 'r2',\n",
    "          cv = 0)\n",
    "\n",
    "sfs.fit(X_total, y)\n",
    "fwrapper_feature = list(sfs.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper Backwards\n",
    "sbs = SFS(svm.SVC(),\n",
    "         k_features=num_features,\n",
    "         forward=False,\n",
    "         floating=False,\n",
    "         cv=0)\n",
    "sbs.fit(X_total, y)\n",
    "bwrapper_feature = list(sbs.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-wise wrapper\n",
    "sffs = SFS(svm.SVC(),\n",
    "         k_features=num_features,\n",
    "         forward=True,\n",
    "         floating=True,\n",
    "         cv=0)\n",
    "sffs.fit(X_total, y)\n",
    "swrapper_feature = list(sffs.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO\n",
    "num_feats = num_features\n",
    "scaler = StandardScaler().fit(X_total)\n",
    "X_train = scaler.transform(X_total)\n",
    "embeded_lr_selector = SelectFromModel(LogisticRegression(C=1, penalty=\"l1\", solver='liblinear'), max_features=num_feats)\n",
    "embeded_lr_selector.fit(scaler.transform(X_total), y)\n",
    "\n",
    "embeded_lr_support = embeded_lr_selector.get_support()\n",
    "embeded_lr_feature = X_total.loc[:,embeded_lr_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "scaler = StandardScaler().fit(X_total)\n",
    "X_train = scaler.transform(X_total)\n",
    "\n",
    "embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_features)\n",
    "embeded_rf_selector.fit(X_total, y)\n",
    "\n",
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = X_total.loc[:,embeded_rf_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "\n",
    "embeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats)\n",
    "embeded_lgb_selector.fit(X_total, y)\n",
    "\n",
    "embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "embeded_lgb_feature = X_total.loc[:,embeded_lgb_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 177\n",
      "Filter:\n",
      "\t pearson correlation: 25\n",
      "\t chi-squared: 25\n",
      "Wrapper\n",
      "\t forward wrapper: 25\n",
      "\t backwards wrapper: 25\n",
      "\t step-wise wrapper: 25\n",
      "Embedded\n",
      "\t Lasso: 25\n",
      "\t Random Forest: 25\n",
      "\t lightGBM: 25\n"
     ]
    }
   ],
   "source": [
    "# Lists of features and printing the amount of features selected\n",
    "selected_features = [ pc_features, chi_feature, fwrapper_feature, bwrapper_feature, swrapper_feature, \n",
    "                    embeded_lr_feature, embeded_rf_feature, embeded_lgb_feature]\n",
    "feature_sel_name = [\"pearson correlation\", \"chi-squared\", \"forward wrapper\", \"backwards wrapper\", \n",
    "                  \"step-wise wrapper\", \"Lasso\", \"Random Forest\", 'lightGBM']\n",
    "\n",
    "print(f\"total features: {len(total_features.keys())}\")\n",
    "print(\"Filter:\")\n",
    "for i in range(len(selected_features)):\n",
    "    print(f\"\\t {feature_sel_name[i]}: {len(selected_features[i])}\")\n",
    "    if feature_sel_name[i] == \"chi-squared\":\n",
    "        print(\"Wrapper\")\n",
    "    if feature_sel_name[i] == \"step-wise wrapper\":\n",
    "        print(\"Embedded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_func(X_train, X_test, y_train, y_test):\n",
    "    # define the model\n",
    "    logistic_model = LogisticRegression(solver='lbfgs', max_iter=100)\n",
    "\n",
    "    # fit/train the model on all features\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "    # Y_test_pred = logistic_model.predict(X_test)\n",
    "\n",
    "    # # score\n",
    "    # log_score = logistic_model.score(X_test,y_test)\n",
    "    return logistic_model #, Y_test_pred, log_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_func(X_train, X_test, y_train, y_test):\n",
    "    # define the model\n",
    "    svm_model = svm.SVC()\n",
    "\n",
    "    # fit/train the model on all features\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    # Y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "    # # score\n",
    "    # svm_score = svm_model.score(X_test,y_test)\n",
    "    return svm_model #, Y_test_pred, svm_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest_func(X_train, X_test, y_train, y_test):\n",
    "    # define the model\n",
    "    randomf_model = RandomForestClassifier()\n",
    "\n",
    "    # fit/train the model on all features\n",
    "    randomf_model.fit(X_train, y_train)\n",
    "    # Y_test_pred = randomf_model.predict(X_test)\n",
    "\n",
    "    # # score\n",
    "    # rf_score=randomf_model.score(X_test, y_test)\n",
    "    return randomf_model #, Y_test_pred, rf_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralnetworks_func(X_train, X_test, Y_train, Y_test):\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Scaling the data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    x_train = scaler.transform(X_train)\n",
    "    x_test = scaler.transform(X_test)\n",
    "    ## Convert labels to categorical\n",
    "    y_train_cat = to_categorical(Y_train)\n",
    "    y_test_cat = to_categorical(Y_test)\n",
    "\n",
    "    # Define the feedforward neural network\n",
    "    neural_model = Sequential()\n",
    "    neural_model.add(Dense(64, activation='relu'))\n",
    "    neural_model.add(Dense(64, activation='relu'))\n",
    "    neural_model.add(Dense(64, activation='relu'))\n",
    "    neural_model.add(Dense(64, activation = 'relu'))\n",
    "    neural_model.add(Dropout(0.5))\n",
    "    neural_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    neural_model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    # history = model.fit(X_train_pca, y_train, validation_data=(X_test_pca, y_test), epochs=45, batch_size=100, verbose=2)\n",
    "    history = neural_model.fit(x_train, y_train_cat, validation_data=(x_test, y_test_cat), epochs=45, batch_size=100, verbose=0)\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    val_accuracies.append(val_accuracy) # accuracy score\n",
    "\n",
    "    # print(val_accuracies)\n",
    "    avg_val_accuracy = sum(val_accuracies) / len(val_accuracies) # average of accurracy scores\n",
    "    # print(\"Average Validation Accuracy:\", avg_val_accuracy)\n",
    "\n",
    "    # plt.plot(history.history['accuracy'])\n",
    "    # plt.plot(history.history['val_accuracy'])\n",
    "    # plt.legend(['accuracy', 'val_accuracy'])\n",
    "    # print(neural_model.predict(X_test))\n",
    "    return val_accuracies\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AdaBoost classifier\n",
    "def adaboost_func(X_train, X_test, Y_train, Y_test):\n",
    "\n",
    "        # Create adaboost classifer object\n",
    "    abc= AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=5)\n",
    "\n",
    "    # Train Adaboost Classifer\n",
    "    adaboost_model = abc.fit(X_train, Y_train)\n",
    "\n",
    "    return adaboost_model\n",
    "    #Predict the response for test dataset\n",
    "    # y_pred = model1.predict(X_test)\n",
    "        # score30= adaboost_model.score(X_test, y_test)\n",
    "        # Ada_scores[i] = score30\n",
    "    # print(Ada_scores)\n",
    "    # print(np.mean(Ada_scores))\n",
    "    #predictions = adaboost_model.predict(X_test)\n",
    "    #print(predictions != y_test)\n",
    "    #print(score30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ada = []\n",
    "trainscores_ada = []\n",
    "for i in range (50):\n",
    "      dtree = DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=1)\n",
    "      adbclassifier = AdaBoostClassifier(base_estimator=dtree,\n",
    "                                    n_estimators=100,\n",
    "                                    learning_rate=1,\n",
    "                                    algorithm = 'SAMME',\n",
    "                                    random_state=1)\n",
    "      X_train, X_test, y_train, y_test = train_test_split(x_train, y, test_size=0.2, random_state= i+1)\n",
    "      adbclassifier.fit(X_train, y_train)\n",
    "      scores_ada.append(adbclassifier.score(X_test,y_test))\n",
    "      trainscores_ada.append(adbclassifier.score(X_train,y_train))\n",
    "print(np.mean(scores_ada))\n",
    "print(len(X_test))\n",
    "plt.ylim(0,1)\n",
    "plt.plot(scores_ada)\n",
    "plt.plot(trainscores_ada)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_func(X_train, X_test, Y_train, Y_test):\n",
    "    label = LabelEncoder()\n",
    "    y2 = label.fit_transform(Y_train)\n",
    "    y2 = list(y2)\n",
    "\n",
    "    xgb_clf = XGBClassifier(n_estimators = 500,\n",
    "                            learning_rate = 1,\n",
    "                            eval_metric = \"logloss\",\n",
    "                            early_stopping_rounds = 5,\n",
    "                            n_jobs = -1,\n",
    "                            )\n",
    "    xgb_clf.fit(X_train, y2,                    \n",
    "                eval_set = [(X_test,Y_test)],\n",
    "                verbose = False)\n",
    "    pred_test = xgb_clf.predict(X_test)\n",
    "    test_score = accuracy_score(pred_test, Y_test)\n",
    "\n",
    "    return test_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which gives predicted and correct values with missclassifications and scores for rest models\n",
    "def model_predict(X_test, y_test1, model):\n",
    "    predictions = model.predict(X_test)\n",
    "    miss_class = np.where(predictions != y_test1)\n",
    "    miss_class = miss_class[0]\n",
    "    score=model.score(X_test, y_test1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_predict_neural(X_test, y_test1, model):\n",
    "#     predictions = model.predict(X_test)\n",
    "#     predictions = np.argmax(predictions, axis = 1)\n",
    "#     y_test1 = y_test1.astype('int64')\n",
    "#     testYarg = np.argmax(y_test1, axis = 1)\n",
    "#     miss_class = np.where(predictions != testYarg)\n",
    "#     miss_class = miss_class[0]\n",
    "#     print(\"Neural network\")\n",
    "#     print(\"Predicted:\",predictions)\n",
    "#     print(\"Correct:  \",testYarg)\n",
    "#     print(\"Index missclassified:\", miss_class)\n",
    "#     # print(\"Score:\", val_accuracy, \"\\n\")\n",
    "#     return predictions, miss_class, testYarg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_test_cat \u001b[39m=\u001b[39m to_categorical(y_test)\n\u001b[0;32m      2\u001b[0m y_test1 \u001b[39m=\u001b[39m y_test_cat\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m testYarg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_test1, axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# y_test_cat = to_categorical(y_test)\n",
    "# y_test1 = y_test_cat.astype('int64')\n",
    "# testYarg = np.argmax(y_test1, axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLogisticRegression()\tSVC()\tRandomForestClassifier()\tAdaBoostClassifier(learning_rate=1, random_state=5)\tSequential()\tLabelEncoder()\n",
      "variance threshold\t0.90311+-0.085\t0.89683+-0.103\t0.89258+-0.086\t0.85659+-0.117\t0.89654+-0.101\t0.03683+-0.082\n",
      "pearson correlation\t0.91013+-0.076\t0.91539+-0.083\t0.89221+-0.100\t0.86647+-0.118\t0.87382+-0.088\t0.08292+-0.126\n",
      "chi-squared\t0.90069+-0.082\t0.91301+-0.087\t0.90253+-0.091\t0.84920+-0.125\t0.91440+-0.087\t0.05164+-0.082\n",
      "forward wrapper\t0.87243+-0.118\t0.86299+-0.088\t0.86649+-0.103\t0.83321+-0.133\t0.85941+-0.115\t0.07707+-0.113\n",
      "backwards wrapper\t0.84681+-0.112\t0.83705+-0.106\t0.82942+-0.107\t0.81053+-0.088\t0.79503+-0.089\t0.09691+-0.082\n",
      "step-wise wrapper\t0.87243+-0.118\t0.86299+-0.088\t0.86854+-0.090\t0.83321+-0.133\t0.86986+-0.093\t0.07707+-0.113\n",
      "Lasso\t0.95385+-0.045\t0.92857+-0.084\t0.89771+-0.100\t0.88200+-0.130\t0.93385+-0.067\t0.05799+-0.127\n",
      "Random Forest\t0.89792+-0.103\t0.91768+-0.086\t0.90217+-0.100\t0.87202+-0.125\t0.91860+-0.071\t0.05168+-0.109\n",
      "lightGBM\t0.73425+-0.060\t0.75165+-0.029\t0.73598+-0.068\t0.74865+-0.046\t0.67668+-0.085\t0.12572+-0.082\n"
     ]
    }
   ],
   "source": [
    "subjects = ['S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S13','S14','S15','S16','S17']\n",
    "model_names = [\"LogisticRegression()\", \"SVC()\", \"RandomForestClassifier()\", \"AdaBoostClassifier(learning_rate=1, random_state=5)\", \"Sequential()\",\n",
    "               \"LabelEncoder()\"]\n",
    "\n",
    "first_line = \"\"\n",
    "for model in model_names:\n",
    "    first_line += \"\\t\" + model\n",
    "print(first_line)\n",
    "\n",
    "for i in range(len(selected_features)):\n",
    "    feat = selected_features[i]\n",
    "    scores_dict = dict()\n",
    "    for model in model_names:\n",
    "        scores_dict[model] = []\n",
    "\n",
    "    for subject in subjects:\n",
    "        X_test = total_features[total_features['index'].str.startswith(subject)]\n",
    "        y_test = X_test['y']\n",
    "        X_test = X_test.drop(['y', 'index'], axis=1)\n",
    "        X_train = total_features[~total_features['index'].str.startswith(subject)]\n",
    "        y_train = X_train['y']\n",
    "        X_train = X_train.drop(['y', 'index'], axis=1)\n",
    "        X_train = X_train[list(feat)]\n",
    "        X_test = X_test[list(feat)]\n",
    "        \n",
    "        # fit the data\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Get models\n",
    "        lg_model = logistic_func(X_train, X_test, y_train, y_test)\n",
    "        svm_model = svm_func(X_train, X_test, y_train, y_test)\n",
    "        rf_model = randomforest_func(X_train, X_test, y_train, y_test)\n",
    "        ada_model = adaboost_func(X_train, X_test, y_train, y_test)\n",
    "        nn_accurary = neuralnetworks_func(X_train, X_test, y_train, y_test)\n",
    "        xgb_accuracy = xgboost_func(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Get the score\n",
    "        models= [lg_model, svm_model, rf_model, ada_model]\n",
    "\n",
    "        for model in models:\n",
    "            score = model_predict(X_test, y_test, model)\n",
    "            scores_dict[str(model)].append(model_predict(X_test, y_test, model))\n",
    "        scores_dict[\"Sequential()\"].append(nn_accurary)\n",
    "        scores_dict[\"LabelEncoder()\"].append(xgb_accuracy)\n",
    "\n",
    "    line = str(feature_sel_name[i])\n",
    "\n",
    "    for a in scores_dict:\n",
    "        score = scores_dict[a]\n",
    "        line += \"\\t\" + f\"{np.mean(score):.5f}\" + \"+-\" + f\"{np.std(score):.3f}\"\n",
    "\n",
    "    # line += \"\\t\" + f\"{np.mean(nn_accurary):.5f}\" + \"+-\" + f\"{np.std(nn_accurary):.3f}\"\n",
    "    print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject-based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7595789473684211 0.029213260501086486\n",
      "0.7560584795321638 0.03490718859492099\n",
      "0.7578713450292397 0.045916981691085046\n",
      "0.7241520467836255 0.039160507713583356\n",
      "0.7356842080752055 0.03299481856033999\n",
      "0.14500584795321636 0.038223066837712226\n"
     ]
    }
   ],
   "source": [
    "sb_dict = dict()\n",
    "feat = selected_features[-1]\n",
    "\n",
    "for model in model_names:\n",
    "    sb_dict[model] = []\n",
    "    \n",
    "for subject in subjects:\n",
    "    X_sb = total_features[~total_features['index'].str.startswith(subject)]\n",
    "    y_sb = X_sb['y']\n",
    "    X_sb = X_sb.drop(['y', 'index'], axis=1)\n",
    "    X_sb = X_sb[list(feat)]\n",
    "\n",
    "    X_train_sb, X_test_sb, y_train_sb, y_test_sb = train_test_split(X_sb, y_sb, test_size=0.2, random_state=5)\n",
    "\n",
    "    scaler_sb = StandardScaler().fit(X_train_sb)\n",
    "    X_train_sb = scaler_sb.transform(X_train_sb)\n",
    "    X_test_sb = scaler_sb.transform(X_test_sb)\n",
    "\n",
    "    lg_model_sb = logistic_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "    svm_model_sb = svm_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "    rf_model_sb = randomforest_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "    # nn_model = neuralnetworks_func(X_train, X_test, Y_train, Y_test)\n",
    "    ada_model_sb = adaboost_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "    nn_accurary_sb = neuralnetworks_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "    xgb_accuracy_sb = xgboost_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "    # Get the score\n",
    "    models= [lg_model_sb, svm_model_sb, rf_model_sb, ada_model_sb]\n",
    "\n",
    "    for model in models:\n",
    "        score = model_predict(X_test_sb, y_test_sb, model)\n",
    "        sb_dict[str(model)].append(model_predict(X_test_sb, y_test_sb, model))\n",
    "    sb_dict[\"Sequential()\"].append(nn_accurary_sb)\n",
    "    sb_dict[\"LabelEncoder()\"].append(xgb_accuracy_sb)\n",
    "\n",
    "    # for model in models:\n",
    "    #     sb_dict[str(model)].append(model_predict(X_test_sb, y_test_sb, model))\n",
    "    \n",
    "for a in sb_dict:\n",
    "    print(np.mean(sb_dict[a]), np.std(sb_dict[a]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S13','S14','S15','S16','S17']\n",
    "model_names = [\"LogisticRegression()\", \"SVC()\", \"RandomForestClassifier()\", \"AdaBoostClassifier(learning_rate=1, random_state=5)\", \"Sequential()\",\n",
    "               \"LabelEncoder()\"]\n",
    "\n",
    "first_line = \"\"\n",
    "for model in model_names:\n",
    "    first_line += \"\\t\" + model\n",
    "print(first_line)\n",
    "\n",
    "x_trains = []\n",
    "for subject in subjects:\n",
    "    X_h = total_features[total_features['index'].str.startswith(subject)]\n",
    "    scaler_hy = StandardScaler().fit(features)\n",
    "    x_trains.append(scaler_hy.transform(features))\n",
    "\n",
    "\n",
    "for i in range(len(selected_features)):\n",
    "    feat = selected_features[i]\n",
    "    scores_dict = dict()\n",
    "\n",
    "    for model in model_names:\n",
    "        scores_dict[model] = []\n",
    "\n",
    "    for subject in subjects:\n",
    "        X_test = total_features[total_features['index'].str.startswith(subject)]\n",
    "        y_test = X_test['y']\n",
    "        X_test = X_test.drop(['y', 'index'], axis=1)\n",
    "        X_train = total_features[~total_features['index'].str.startswith(subject)]\n",
    "        y_train = X_train['y']\n",
    "        X_train = X_train.drop(['y', 'index'], axis=1)\n",
    "        X_train = X_train[list(feat)]\n",
    "        X_test = X_test[list(feat)]\n",
    "        \n",
    "        # fit the data\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Get models\n",
    "        lg_model = logistic_func(X_train, X_test, y_train, y_test)\n",
    "        svm_model = svm_func(X_train, X_test, y_train, y_test)\n",
    "        rf_model = randomforest_func(X_train, X_test, y_train, y_test)\n",
    "        ada_model = adaboost_func(X_train, X_test, y_train, y_test)\n",
    "        nn_accurary = neuralnetworks_func(X_train, X_test, y_train, y_test)\n",
    "        xgb_accuracy = xgboost_func(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Get the score\n",
    "        models= [lg_model, svm_model, rf_model, ada_model]\n",
    "\n",
    "        for model in models:\n",
    "            score = model_predict(X_test, y_test, model)\n",
    "            scores_dict[str(model)].append(model_predict(X_test, y_test, model))\n",
    "        scores_dict[\"Sequential()\"].append(nn_accurary)\n",
    "        scores_dict[\"LabelEncoder()\"].append(xgb_accuracy)\n",
    "\n",
    "    line = str(feature_sel_name[i])\n",
    "\n",
    "    for a in scores_dict:\n",
    "        score = scores_dict[a]\n",
    "        line += \"\\t\" + f\"{np.mean(score):.5f}\" + \"+-\" + f\"{np.std(score):.3f}\"\n",
    "\n",
    "    # line += \"\\t\" + f\"{np.mean(nn_accurary):.5f}\" + \"+-\" + f\"{np.std(nn_accurary):.3f}\"\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
