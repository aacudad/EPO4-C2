{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\antropy\\fractal.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit('float64(float64[:], int32)')\n"
     ]
    }
   ],
   "source": [
    "import ipynb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from scipy import signal\n",
    "# import neurokit2 as nk\n",
    "import random\n",
    "# %matplotlib inline \n",
    "import pyhrv\n",
    "import ipynb\n",
    "\n",
    "from ipynb.fs.full.ECG_features import get_ecgfeatures\n",
    "from ipynb.fs.full.EDA import get_eda_features_and_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class read_data_of_one_subject:\n",
    "            \"\"\"Read data from WESAD dataset\"\"\"\n",
    "            def __init__(self, path, subject):\n",
    "                self.keys = ['label', 'subject', 'signal']\n",
    "                self.signal_keys = ['wrist', 'chest']\n",
    "                self.chest_sensor_keys = ['ACC', 'ECG', 'EDA', 'EMG', 'Resp', 'Temp']\n",
    "                self.wrist_sensor_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "                #os.chdir(path)\n",
    "                #os.chdir(subject)\n",
    "                with open(path + subject +'/'+subject + '.pkl', 'rb') as file:\n",
    "                    data = pickle.load(file, encoding='latin1')\n",
    "                self.data = data\n",
    "\n",
    "            def get_labels(self):\n",
    "                return self.data[self.keys[0]]\n",
    "\n",
    "            def get_wrist_data(self):\n",
    "                \"\"\"\"\"\"\n",
    "                #label = self.data[self.keys[0]]\n",
    "                assert subject == self.data[self.keys[1]]\n",
    "                signal = self.data[self.keys[2]]\n",
    "                wrist_data = signal[self.signal_keys[0]]\n",
    "                #wrist_ACC = wrist_data[self.wrist_sensor_keys[0]]\n",
    "                #wrist_ECG = wrist_data[self.wrist_sensor_keys[1]]\n",
    "                return wrist_data\n",
    "\n",
    "            def get_chest_data(self):\n",
    "                \"\"\"\"\"\"\n",
    "                signal = self.data[self.keys[2]]\n",
    "                chest_data = signal[self.signal_keys[1]]\n",
    "                return chest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2\n",
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_eda_features_and_index() missing 1 required positional argument: 'first'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m ecg_base\u001b[39m=\u001b[39mchest_data_dict[\u001b[39m'\u001b[39m\u001b[39mECG\u001b[39m\u001b[39m'\u001b[39m][baseline,\u001b[39m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m resp_base\u001b[39m=\u001b[39mchest_data_dict[\u001b[39m'\u001b[39m\u001b[39mResp\u001b[39m\u001b[39m'\u001b[39m][baseline,\u001b[39m0\u001b[39m]\n\u001b[1;32m---> 41\u001b[0m edafeatures, start, end \u001b[39m=\u001b[39m get_eda_features_and_index(eda_base, fs, subject, label, edafeatures)\n\u001b[0;32m     43\u001b[0m \u001b[39mprint\u001b[39m(start, end)\n\u001b[0;32m     45\u001b[0m ecg \u001b[39m=\u001b[39m ecg_base[start: end]\n",
      "\u001b[1;31mTypeError\u001b[0m: get_eda_features_and_index() missing 1 required positional argument: 'first'"
     ]
    }
   ],
   "source": [
    "ecgfeatures = pd.DataFrame(None, columns= ['feat_rmssd', 'feat_nnhr', \n",
    "                                    'feat_sdsd', 'feat_pnn50', 'feat_pnn20', 'peak vlf', 'peak lf','peak hf', 'norm power lf', \n",
    "                                    'norm power hf', 'power ratio', 'total power'])\n",
    "edafeatures = None\n",
    "\n",
    "fs = 700\n",
    "y = []\n",
    "data_set_path= \"C:/Users/riche/Downloads/WESAD/WESAD/\" # Folder path\n",
    "\n",
    "for i in range(16): # 16\n",
    "    subject = 'S'+str(i+2) # Cycle through S2 to S17\n",
    "    print(subject)\n",
    "    if subject != 'S12': # Skip S12, because it does nog exist\n",
    "        for label in range(1,5):\n",
    "\n",
    "            print(label)\n",
    "            if label==3 or label==4:\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(label)\n",
    "\n",
    "            # Object instantiation\n",
    "            obj_data = {}\n",
    "\n",
    "            # Accessing class attributes and method through objects\n",
    "            obj_data[subject] = read_data_of_one_subject(data_set_path, subject)\n",
    "\n",
    "            chest_data_dict = obj_data[subject].get_chest_data()\n",
    "            chest_dict_length = {key: len(value) for key, value in chest_data_dict.items()}\n",
    "            #print(chest_dict_length)\n",
    "\n",
    "            # Get labels\n",
    "            labels = obj_data[subject].get_labels()\n",
    "            baseline = np.asarray([idx for idx,val in enumerate(labels) if val == label])\n",
    "            #plt.plot(labels)\n",
    "\n",
    "            eda_base=chest_data_dict['EDA'][baseline,0] # Select the EDA data\n",
    "            ecg_base=chest_data_dict['ECG'][baseline,0]\n",
    "            resp_base=chest_data_dict['Resp'][baseline,0]\n",
    "\n",
    "            edafeatures, start, end = get_eda_features_and_index(eda_base, fs, subject, label, edafeatures)\n",
    "\n",
    "            print(start, end)\n",
    "\n",
    "            ecg = ecg_base[start: end]\n",
    "            resp = ecg_base[start: end]\n",
    "\n",
    "            ecgfeatures = get_ecgfeatures(ecg, fs, subject, label, ecgfeatures)\n",
    "            # t=np.arange(0,eda_base.size*(1/fs),(1/fs))\n",
    "            # t=t[:eda_base.size]\n",
    "            \n",
    "            # plt.figure(figsize=(12,4))\n",
    "            # plt.plot(t, eda_base)\n",
    "            # plt.xlabel('$Time (s)$') \n",
    "            # plt.ylabel('$signal$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "edafeatures.to_csv(\"EDA_features.csv\")\n",
    "ecgfeatures.to_csv(\"ECG_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([ecgfeatures, edafeatures])\n",
    "features.to_csv(\"out_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_ecgfeatures\n",
    "print(len(y), df_features.shape)\n",
    "df_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score=[]\n",
    "# print(df_features.shape)\n",
    "# for i in range (1,50):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.3, random_state=i)\n",
    "#     scaler = StandardScaler().fit(X_train)\n",
    "#     X_train = scaler.transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "#     pca = PCA(n_components=7)\n",
    "#     pca =pca.fit(X_train)\n",
    "#     X_train = pca.transform(X_train)\n",
    "#     X_test = pca.transform(X_test)\n",
    "#     model = LogisticRegression()\n",
    "#     model.fit(X_train, y_train)\n",
    "#     score.append(model.score(X_test, y_test))\n",
    "# print(score)\n",
    "# print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.3, random_state=4)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=12)\n",
    "pca.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c=y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_pca, y_train)\n",
    "score = logistic_model.score(X_test_pca,y_test)\n",
    "print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(X_train_pca, y_train)\n",
    "score = svm_model.score(X_test_pca,y_test)\n",
    "print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=12)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Define the feedforward neural network\n",
    "neural_model = Sequential()\n",
    "neural_model.add(Dense(64, activation='relu', input_shape=(14,)))\n",
    "neural_model.add(Dense(64, activation='relu'))\n",
    "neural_model.add(Dense(32, activation='relu'))\n",
    "neural_model.add(Dense(100, activation = 'relu'))\n",
    "neural_model.add(Dropout(0.5))\n",
    "neural_model.add(Dense(3, activation='softmax'))\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "neural_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit(X_train_pca, y_train, validation_data=(X_test_pca, y_test), epochs=45, batch_size=100, verbose=2)\n",
    "val_accuracies = []\n",
    "for i in range(1):\n",
    "    # ... your code ...\n",
    "    history = neural_model.fit(X_train_pca, y_train, validation_data=(X_test_pca, y_test), epochs=45, batch_size=100, verbose=0)\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "avg_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n",
    "print(\"Average Validation Accuracy:\", avg_val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['accuracy', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(X_test, y_test, model):\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = np.argmax(predictions, axis = 1)\n",
    "    y_test1 = y_test.astype('int64')\n",
    "    testYarg = np.argmax(y_test1, axis = 1)\n",
    "    miss_class = np.where(predictions - testYarg !=0)\n",
    "    miss_class = miss_class[0]\n",
    "    return predictions, miss_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict(X_test_pca, y_test, neural_model)\n",
    "print(miss_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# define the model\n",
    "randomf_model = RandomForestClassifier()\n",
    "\n",
    "# fit/train the model on all features\n",
    "randomf_model.fit(X_train_pca, y_train)\n",
    "\n",
    "#score\n",
    "score=randomf_model.score(X_test_pca, y_test)\n",
    "\n",
    "# get feature importance\n",
    "importance = randomf_model.feature_importances_\n",
    "\n",
    "#creat a dictionary with key=indices, and values=importance\n",
    "important_features_dict = {}\n",
    "for idx, val in enumerate(importance):\n",
    "    important_features_dict[idx] = val\n",
    "# sorting \n",
    "important_features_list = sorted(important_features_dict,\n",
    "                                 key=important_features_dict.get,\n",
    "                                 reverse=True)\n",
    "# print indices of top 5 features\n",
    "print(important_features_list[:5])\n",
    "plt.plot(importance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation score of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def cross_validation(model, X, y):\n",
    "    cv = KFold(n_splits= 5, random_state=1, shuffle=True)\n",
    "    scores = cross_val_score(model, X,y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_logistic = cross_validation(df_ecgfeatures, y, logistic_model)\n",
    "cv_neural = cross_validation(df_ecgfeatures, y, neural_model)\n",
    "cv_randomf = cross_validation(df_ecgfeatures, y, randomf_model)\n",
    "print(cv_logistic)\n",
    "print(cv_neural)\n",
    "print(cv_randomf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
