{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\antropy\\fractal.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit('float64(float64[:], int32)')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import neurokit2 as nk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pyhrv.time_domain as td\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pyhrv\n",
    "import ipynb\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# functions for features extraction\n",
    "from ipynb.fs.full.ECG_features import get_ecgfeatures\n",
    "from ipynb.fs.full.EDA import get_edaindex, get_edafeatures\n",
    "from ipynb.fs.full.BVP import get_bvpfeatures, bvp_prep, sensorPPGfilter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection & Feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset reading\n",
    "class read_data_of_one_subject:\n",
    "            \"\"\"Read data from WESAD dataset\"\"\"\n",
    "            def __init__(self, path, subject):\n",
    "                self.keys = ['label', 'subject', 'signal']\n",
    "                self.signal_keys = ['wrist', 'chest']\n",
    "                self.chest_sensor_keys = ['ACC', 'ECG', 'EDA', 'EMG', 'Resp', 'Temp']\n",
    "                self.wrist_sensor_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "                #os.chdir(path)\n",
    "                #os.chdir(subject)\n",
    "                with open(path + subject +'/'+subject + '.pkl', 'rb') as file:\n",
    "                    data = pickle.load(file, encoding='latin1')\n",
    "                self.data = data\n",
    "\n",
    "            def get_labels(self):\n",
    "                return self.data[self.keys[0]]\n",
    "\n",
    "            def get_wrist_data(self):\n",
    "                \"\"\"\"\"\"\n",
    "                #label = self.data[self.keys[0]]\n",
    "                #assert subject == self.data[self.keys[1]]\n",
    "                signal = self.data[self.keys[2]]\n",
    "                wrist_data = signal[self.signal_keys[0]]\n",
    "                #wrist_ACC = wrist_data[self.wrist_sensor_keys[0]]\n",
    "                #wrist_ECG = wrist_data[self.wrist_sensor_keys[1]]\n",
    "                return wrist_data\n",
    "\n",
    "            def get_chest_data(self):\n",
    "                \"\"\"\"\"\"\n",
    "                signal = self.data[self.keys[2]]\n",
    "                chest_data = signal[self.signal_keys[1]]\n",
    "                return chest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "# Set up empty dataframes for the features\n",
    "columns_ecg =['index', 'ecg_HR_mean', 'ecg_HR_min', 'ecg_HR_max', 'ecg_HR_std', 'ecg_SDNN', 'ecg_SDANN', 'ecg_RMSSD', \n",
    "                                           'ecg_SDSD','ecg_pNN50', 'ecg_pNN20', \"ecg_triangular_index\", \"ecg_tinn\", \"ecg_sd1\", \"ecg_sd2\",\n",
    "                                            \"ecg_ratio_sd2_sd1\", 'ecg_abs_power_VLF', 'ecg_abs_power_LF', 'ecg_abs_power_HF', 'ecg_tot_power',\n",
    "                                            'ecg_LF/HF', 'ecg_peak_vlf', 'ecg_peak_lf', 'ecg_peak_hf', 'ecg_norm_power_LF', 'ecg_norm_power_HF',\n",
    "                                            'max_cwt_absmean', 'min_cwt_absmean', 'mean_cwt_absmean','std_cwt_absmean', \n",
    "                                            'max_cwt_std', 'min_cwt_std', 'mean_cwt_std','std_cwt_std',\n",
    "                                            'max_cwt_energy', 'min_cwt_energy', 'mean_cwt_energy','std_cwt_energy',\n",
    "                                            'max_cwt_var', 'min_cwt_var', 'mean_cwt_var','std_cwt_var',\n",
    "                                            'entropy']\n",
    "columns_resp = ['RSP_Rate_Mean','RRV_RMSSD', 'RRV_MeanBB', 'RRV_SDBB', 'RRV_SDSD', 'RRV_CVBB', 'RRV_CVSD', 'RRV_MedianBB',\n",
    "                'RRV_MadBB', 'RRV_MCVBB', 'RRV_VLF', 'RRV_LF', 'RRV_HF', 'RRV_LFHF', 'RRV_LFn', 'RRV_HFn', 'RRV_SD1',\n",
    "                'RRV_SD2', 'RRV_SD2SD1', 'RRV_ApEn', 'RRV_SampEn', 'RSP_Amplitude_Mean', 'RSP_RVT', 'RSP_Symmetry_PeakTrough',\n",
    "                'RSP_Symmetry_RiseDecay', 'RSP_Phase_Duration_Inspiration', 'RSP_Phase_Duration_Expiration', 'RSP_Phase_Duration_Ratio']\n",
    "\n",
    "columns = [*columns_ecg, *columns_resp]\n",
    "ecgfeatures = pd.DataFrame(None, columns= columns)\n",
    "edafeatures = None\n",
    "bvpfeatures = None\n",
    "\n",
    "print(len(ecgfeatures.keys()))\n",
    "total_samples = 0\n",
    "fs = 700 # sampling frequency\n",
    "fs_bvp = 64\n",
    "duration = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: S2\n",
      "total samples S2: 29\n",
      "subject: S3\n",
      "total samples S3: 29\n",
      "subject: S4\n",
      "total samples S4: 29\n",
      "subject: S5\n",
      "total samples S5: 29\n",
      "subject: S6\n",
      "total samples S6: 29\n",
      "subject: S7\n",
      "total samples S7: 29\n",
      "subject: S8\n",
      "total samples S8: 29\n",
      "subject: S9\n",
      "total samples S9: 29\n",
      "subject: S10\n",
      "total samples S10: 30\n",
      "subject: S11\n",
      "total samples S11: 30\n",
      "subject: S12\n",
      "total samples S12: 0\n",
      "subject: S13\n",
      "total samples S13: 29\n",
      "subject: S14\n",
      "total samples S14: 30\n",
      "subject: S15\n",
      "total samples S15: 30\n",
      "subject: S16\n",
      "total samples S16: 29\n",
      "subject: S17\n",
      "total samples S17: 29\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "amountpsample = dict()\n",
    "# data_set_path= \"C:/Users/JackC/Documents/EPO4/WESAD/WESAD/\" # Folder path\n",
    "data_set_path= \"C:/Users/riche/Downloads/WESAD/WESAD/\" # Folder path\n",
    "# data_set_path = \"C:/Users/Adnane/Downloads/WESAD/WESAD/\"\n",
    "\n",
    "for i in range(16): # 15 subjects\n",
    "    subject = 'S'+str(i+2) # Cycle through S2 to S17\n",
    "    print(f\"subject: {subject}\")\n",
    "    amountpsample[subject] = 0\n",
    "    if subject != 'S12': # Skip S12, because it does not exist\n",
    "        # Object instantiation\n",
    "        obj_data = {}\n",
    "\n",
    "        # Accessing class attributes and method through objects\n",
    "        obj_data[subject] = read_data_of_one_subject(data_set_path, subject)\n",
    "\n",
    "        chest_data_dict = obj_data[subject].get_chest_data()\n",
    "        wrist_data_dict = obj_data[subject].get_wrist_data()\n",
    "        chest_dict_length = {key: len(value) for key, value in chest_data_dict.items()}\n",
    "        wrist_dict_length = {key: len(value) for key, value in wrist_data_dict.items()}\n",
    "\n",
    "        # Get labels\n",
    "        labels = obj_data[subject].get_labels()\n",
    "\n",
    "        for label in [1,2,4]: # for every state of stress\n",
    "            \n",
    "            baseline = np.asarray([idx for idx,val in enumerate(labels) if val == label])\n",
    "\n",
    "            # Obtaining the chest data\n",
    "            eda_base=chest_data_dict['EDA'][baseline,0] # Select the EDA data\n",
    "            ecg_base=chest_data_dict['ECG'][baseline,0] # Select the ECG data\n",
    "            resp_base=chest_data_dict['Resp'][baseline,0] # Select the respiration data\n",
    "\n",
    "            # Obtaining the wrist data\n",
    "            baseline_BVP = baseline * fs_bvp // fs\n",
    "            bvp_base=wrist_data_dict['BVP'][baseline_BVP,0] # Select the BVP data\n",
    "\n",
    "            # Multiple smaller samples per signal\n",
    "            # samples = 10 # specify amount of samples\n",
    "\n",
    "            for j in range(0, len(eda_base)//(duration*fs), 2): # loop over all segments\n",
    "                amountpsample[subject] += 1\n",
    "                # adding the labels to y\n",
    "                if label==1 or label==4: # if baseline or medidation, classify as non-stress\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                # Cutting the signal into a segment\n",
    "                start, end = j*fs*duration, (j+1)*fs*duration\n",
    "                ecg = ecg_base[start: end]\n",
    "                eda = eda_base[start: end]\n",
    "                bvp = bvp_base[start: end]\n",
    "\n",
    "                # splitting sample into 10 smaller even-sized parts\n",
    "                #ecg = ecg_base[int(len(ecg_base)* (j/samples)) : int(len(ecg_base)* ((j+1)/samples))]\n",
    "                #eda = eda_base[int(len(eda_base)* (j/samples)) : int(len(eda_base)* ((j+1)/samples))]\n",
    "\n",
    "                # getting the eda and ecg features\n",
    "                index = subject + str(label) + str(j)\n",
    "                tonic, phasic, start, end = get_edaindex(eda, fs)\n",
    "                edafeatures = get_edafeatures(index, edafeatures, phasic, tonic, fs)\n",
    "                ecgfeatures = get_ecgfeatures(ecg, fs, ecgfeatures, index)\n",
    "                \n",
    "                bvpfeature = get_bvpfeatures(bvp, index)\n",
    "                bvpfeatures = pd.concat([bvpfeatures, bvpfeature], axis=0)\n",
    "    print(f\"total samples {subject}: {amountpsample[subject]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_edafeatures = edafeatures\n",
    "# saved_ecgfeatures = ecgfeatures\n",
    "# saved_bvpfeatures = bvpfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvp_filt = bvp_prep(bvpfeatures) # remove nan and inf in BVP features\n",
    "# ecgfeatures = bvp_prep(ecgfeatures)\n",
    "\n",
    "# saving the feature data in csv files\n",
    "edafeatures.to_csv(\"EDA_features.csv\")\n",
    "ecgfeatures.to_csv(\"ECG_features.csv\")\n",
    "bvp_filt.to_csv(\"BVP_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #reading from csv file if necessary\n",
    "# edafeatures = pd.read_csv(\"EDA_features.csv\", index_col=0)\n",
    "# ecgfeatures = pd.read_csv(\"ECG_features.csv\", index_col=0)\n",
    "# bvp_filt = pd.read_csv(\"BVP_features.csv\", index_col=0)\n",
    "#features = pd.read_csv(\"features_\"+str(samples)+\"_samp.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = pd.merge(ecgfeatures, edafeatures, left_index=True, right_index=True)\n",
    "total_features = pd.merge(bvp_filt, features1, on='index')\n",
    "total_features['y'] = y\n",
    "\n",
    "total_features = total_features[total_features.RSP_RVT != 0.0]\n",
    "total_features.to_csv(\"out_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_total_features = total_features\n",
    "# total_features = pd.read_csv(\"out_features.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = total_features.fillna(method=\"ffill\")\n",
    "total_features = total_features.fillna(method=\"bfill\")\n",
    "total_features = total_features.dropna(axis='columns')\n",
    "features = total_features\n",
    "features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "features = features.dropna(axis='columns')\n",
    "total_features = features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402 402\n"
     ]
    }
   ],
   "source": [
    "# Scaling the data\n",
    "# scaler = StandardScaler().fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "num_features = 25\n",
    "y = total_features['y']\n",
    "X_total = total_features.drop(['y', 'index'], axis=1)\n",
    "print(len(y), len(X_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Threshold\n",
    "scaler = MinMaxScaler().fit(X_total)\n",
    "X_train = scaler.transform(X_total)\n",
    "\n",
    "total_features_index = []\n",
    "for feat in X_total:\n",
    "    total_features_index.append(feat)\n",
    "\n",
    "X_vt = X_train[0:,0:len(X_total.keys())]\n",
    "\n",
    "v_threshold = VarianceThreshold(threshold=0.005) # Set a threshold\n",
    "v_threshold.fit(X_vt)\n",
    "index = v_threshold.get_support()\n",
    "true_index = [i for i, x in enumerate(index) if x]\n",
    "vt_features = [total_features_index[i] for i in true_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pearson correlation\n",
    "def cor_selector(X_cor, y,num_feats):\n",
    "    cor_list = []\n",
    "    feature_name = X_cor.columns.tolist()\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X_cor.columns.tolist():\n",
    "        cor = np.corrcoef(X_cor[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X_cor.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    return cor_support, cor_feature\n",
    "\n",
    "def pears(X):\n",
    "    cor_support, cor_feature = cor_selector(X, y, num_features)\n",
    "    return cor_feature\n",
    "\n",
    "pc_features = pears(X_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi-squared\n",
    "X_norm = MinMaxScaler().fit_transform(X_total)\n",
    "\n",
    "chi_selector = SelectKBest(chi2, k=num_features)\n",
    "chi_selector.fit(X_norm, y)\n",
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X_total.loc[:,chi_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper Forwards\n",
    "sfs = SFS(svm.SVC(),\n",
    "          k_features=num_features,\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring = 'r2',\n",
    "          cv = 0)\n",
    "\n",
    "sfs.fit(X_total, y)\n",
    "fwrapper_feature = list(sfs.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper Backwards\n",
    "sbs = SFS(svm.SVC(),\n",
    "         k_features=num_features,\n",
    "         forward=False,\n",
    "         floating=False,\n",
    "         cv=0)\n",
    "sbs.fit(X_total, y)\n",
    "bwrapper_feature = list(sbs.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-wise wrapper\n",
    "amount_feat = len(total_features.keys())\n",
    "left_bound = (amount_feat//2) - (num_features//2)\n",
    "right_bound =(amount_feat//2) + (num_features//2)\n",
    "\n",
    "sffs = SFS(svm.SVC(),\n",
    "         k_features=(left_bound, right_bound),\n",
    "         forward=True,\n",
    "         floating=True,\n",
    "         cv=0)\n",
    "sffs.fit(X_total, y)\n",
    "swrapper_feature = list(sffs.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO\n",
    "num_feats = num_features\n",
    "scaler = StandardScaler().fit(X_total)\n",
    "X_train = scaler.transform(X_total)\n",
    "embeded_lr_selector = SelectFromModel(LogisticRegression(C=1, penalty=\"l1\", solver='liblinear'), max_features=num_feats)\n",
    "embeded_lr_selector.fit(scaler.transform(X_total), y)\n",
    "\n",
    "embeded_lr_support = embeded_lr_selector.get_support()\n",
    "embeded_lr_feature = X_total.loc[:,embeded_lr_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "scaler = StandardScaler().fit(X_total)\n",
    "X_train = scaler.transform(X_total)\n",
    "\n",
    "embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_features)\n",
    "embeded_rf_selector.fit(X_total, y)\n",
    "\n",
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = X_total.loc[:,embeded_rf_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightBM\n",
    "lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "\n",
    "embeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats)\n",
    "embeded_lgb_selector.fit(X_total, y)\n",
    "\n",
    "embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "embeded_lgb_feature = X_total.loc[:,embeded_lgb_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 175\n",
      "Filter:\n",
      "\t variance threshold: 157\n",
      "\t pearson correlation: 25\n",
      "\t chi-squared: 25\n",
      "Wrapper\n",
      "\t forward wrapper: 25\n",
      "\t backwards wrapper: 25\n",
      "\t step-wise wrapper: 75\n",
      "Embedded\n",
      "\t Lasso: 25\n",
      "\t Random Forest: 25\n",
      "\t lightGBM: 25\n"
     ]
    }
   ],
   "source": [
    "# Lists of features and printing the amount of features selected\n",
    "# vt_features,\n",
    "# \"variance threshold\", \n",
    "selected_features = [vt_features, pc_features, chi_feature, fwrapper_feature, bwrapper_feature, swrapper_feature, \n",
    "                    embeded_lr_feature, embeded_rf_feature, embeded_lgb_feature]\n",
    "feature_sel_name = [\"variance threshold\", \"pearson correlation\", \"chi-squared\", \"forward wrapper\", \"backwards wrapper\", \n",
    "                  \"step-wise wrapper\", \"Lasso\", \"Random Forest\", 'lightGBM']\n",
    "\n",
    "print(f\"total features: {len(total_features.keys())}\")\n",
    "print(\"Filter:\")\n",
    "for i in range(len(selected_features)):\n",
    "    print(f\"\\t {feature_sel_name[i]}: {len(selected_features[i])}\")\n",
    "    if feature_sel_name[i] == \"chi-squared\":\n",
    "        print(\"Wrapper\")\n",
    "    if feature_sel_name[i] == \"step-wise wrapper\":\n",
    "        print(\"Embedded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_func(X_train, X_test, y_train, y_test):\n",
    "    # define the model\n",
    "    logistic_model = LogisticRegression(solver='lbfgs', max_iter=100)\n",
    "\n",
    "    # fit/train the model on all features\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "    # Y_test_pred = logistic_model.predict(X_test)\n",
    "\n",
    "    # # score\n",
    "    # log_score = logistic_model.score(X_test,y_test)\n",
    "    return logistic_model #, Y_test_pred, log_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_func(X_train, X_test, y_train, y_test):\n",
    "    # define the model\n",
    "    svm_model = svm.SVC()\n",
    "\n",
    "    # fit/train the model on all features\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    # Y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "    # # score\n",
    "    # svm_score = svm_model.score(X_test,y_test)\n",
    "    return svm_model #, Y_test_pred, svm_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest_func(X_train, X_test, y_train, y_test):\n",
    "    # define the model\n",
    "    randomf_model = RandomForestClassifier()\n",
    "\n",
    "    # fit/train the model on all features\n",
    "    randomf_model.fit(X_train, y_train)\n",
    "    # Y_test_pred = randomf_model.predict(X_test)\n",
    "\n",
    "    # # score\n",
    "    # rf_score=randomf_model.score(X_test, y_test)\n",
    "    return randomf_model #, Y_test_pred, rf_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralnetworks_func(X_train, X_test, Y_train, Y_test):\n",
    "    val_accuracies = []\n",
    "\n",
    "    ## Convert labels to categorical\n",
    "    y_train_cat = to_categorical(Y_train, 3)\n",
    "    y_test_cat = to_categorical(Y_test, 3)\n",
    "\n",
    "    # Define the feedforward neural network\n",
    "    neural_model = Sequential()\n",
    "    neural_model.add(Dense(64, activation='relu'))\n",
    "    neural_model.add(Dense(64, activation='relu'))\n",
    "    neural_model.add(Dense(64, activation='relu'))\n",
    "    neural_model.add(Dense(64, activation = 'relu'))\n",
    "    neural_model.add(Dropout(0.5))\n",
    "    neural_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    neural_model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    # history = model.fit(X_train_pca, y_train, validation_data=(X_test_pca, y_test), epochs=45, batch_size=100, verbose=2)\n",
    "    history = neural_model.fit(X_train, y_train_cat, validation_data=(X_test, y_test_cat), epochs=45, batch_size=100, verbose=0)\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    val_accuracies.append(val_accuracy) # accuracy score\n",
    "\n",
    "    # print(val_accuracies)\n",
    "    avg_val_accuracy = sum(val_accuracies) / len(val_accuracies) # average of accurracy scores\n",
    "    # print(\"Average Validation Accuracy:\", avg_val_accuracy)\n",
    "\n",
    "    # plt.plot(history.history['accuracy'])\n",
    "    # plt.plot(history.history['val_accuracy'])\n",
    "    # plt.legend(['accuracy', 'val_accuracy'])\n",
    "    # print(neural_model.predict(X_test))\n",
    "    return val_accuracies\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AdaBoost classifier\n",
    "def adaboost_func(X_train, X_test, Y_train, Y_test):\n",
    "\n",
    "        # Create adaboost classifer object\n",
    "    abc= AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=5)\n",
    "\n",
    "    # Train Adaboost Classifer\n",
    "    adaboost_model = abc.fit(X_train, Y_train)\n",
    "\n",
    "    return adaboost_model\n",
    "    #Predict the response for test dataset\n",
    "    # y_pred = model1.predict(X_test)\n",
    "        # score30= adaboost_model.score(X_test, y_test)\n",
    "        # Ada_scores[i] = score30\n",
    "    # print(Ada_scores)\n",
    "    # print(np.mean(Ada_scores))\n",
    "    #predictions = adaboost_model.predict(X_test)\n",
    "    #print(predictions != y_test)\n",
    "    #print(score30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_ada = []\n",
    "# trainscores_ada = []\n",
    "# for i in range (50):\n",
    "#       dtree = DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=1)\n",
    "#       adbclassifier = AdaBoostClassifier(base_estimator=dtree,\n",
    "#                                     n_estimators=100,\n",
    "#                                     learning_rate=1,\n",
    "#                                     algorithm = 'SAMME',\n",
    "#                                     random_state=1)\n",
    "#       X_train, X_test, y_train, y_test = train_test_split(x_train, y, test_size=0.2, random_state= i+1)\n",
    "#       adbclassifier.fit(X_train, y_train)\n",
    "#       scores_ada.append(adbclassifier.score(X_test,y_test))\n",
    "#       trainscores_ada.append(adbclassifier.score(X_train,y_train))\n",
    "# print(np.mean(scores_ada))\n",
    "# print(len(X_test))\n",
    "# plt.ylim(0,1)\n",
    "# plt.plot(scores_ada)\n",
    "# plt.plot(trainscores_ada)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_func(X_train, X_test, Y_train, Y_test):\n",
    "    label = LabelEncoder()\n",
    "    y2 = label.fit_transform(Y_train)\n",
    "    y2 = list(y2)\n",
    "\n",
    "    xgb_clf = XGBClassifier(n_estimators = 500,\n",
    "                            learning_rate = 1,\n",
    "                            eval_metric = \"logloss\",\n",
    "                            early_stopping_rounds = 5,\n",
    "                            n_jobs = -1,\n",
    "                            )\n",
    "    xgb_clf.fit(X_train, y2,                    \n",
    "                eval_set = [(X_test,Y_test)],\n",
    "                verbose = False)\n",
    "    pred_test = xgb_clf.predict(X_test)\n",
    "    test_score = accuracy_score(pred_test, Y_test)\n",
    "\n",
    "    return test_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_ecgfeatures = pd.DataFrame(None, columns= columns)\n",
    "meas_edafeatures = None\n",
    "meas_bvpfeatures = None\n",
    "\n",
    "meas_fs = 400 # sampling frequency\n",
    "meas_y = []\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nicky nultest 1\n",
      "13400\n",
      "20 7149\n",
      "Nicky nultest 2\n",
      "12916\n",
      "20 7149\n",
      "Nicky rest 1\n",
      "95473\n",
      "20 7150\n",
      "7151 14362\n",
      "14362 21573\n",
      "21574 28785\n",
      "28785 35996\n",
      "35997 43208\n",
      "43208 50419\n",
      "50420 57631\n",
      "57631 64843\n",
      "64843 72054\n",
      "72055 79266\n",
      "79266 86477\n",
      "86478 93689\n",
      "Nicky stresstest 1\n",
      "54197\n",
      "20 7149\n",
      "7150 14361\n",
      "14361 21572\n",
      "21573 28784\n",
      "28784 35995\n",
      "35996 43207\n",
      "43207 50418\n",
      "Nicky stresstest 2\n",
      "36158\n",
      "20 7151\n",
      "7152 14363\n",
      "14363 21574\n",
      "21575 28786\n",
      "28786 35997\n",
      "Rick nultest 1\n",
      "13155\n",
      "20 7159\n",
      "Rick nultest 2\n",
      "12129\n",
      "20 7156\n",
      "Rick rest 1\n",
      "84723\n",
      "20 7149\n",
      "7150 14361\n",
      "14361 21572\n",
      "21573 28784\n",
      "28784 35995\n",
      "35996 43207\n",
      "43207 50418\n",
      "50419 57630\n",
      "57630 64842\n",
      "64842 72053\n",
      "72054 79265\n",
      "Rick stresstest 1\n",
      "69518\n",
      "20 7155\n",
      "7156 14367\n",
      "14367 21578\n",
      "21579 28790\n",
      "28790 36001\n",
      "36002 43213\n",
      "43213 50424\n",
      "50425 57636\n",
      "57636 64848\n",
      "Rick stresstest 2\n",
      "53373\n",
      "20 7154\n",
      "7155 14366\n",
      "14366 21577\n",
      "21578 28789\n",
      "28789 36000\n",
      "36001 43212\n",
      "43212 50423\n",
      "Tomasz nultest 1\n",
      "22472\n",
      "20 7148\n",
      "7149 14360\n",
      "14360 21571\n",
      "Tomasz nultest 2\n",
      "17383\n",
      "20 7152\n",
      "7153 14364\n",
      "Tomasz rest 1\n",
      "36019\n",
      "20 7139\n",
      "7140 14351\n",
      "14351 21562\n",
      "21563 28774\n",
      "28774 35985\n",
      "Tomasz rest 2\n",
      "44729\n",
      "20 7158\n",
      "7159 14370\n",
      "14370 21581\n",
      "21582 28793\n",
      "28793 36004\n",
      "36005 43216\n",
      "Tomasz stresstest 1\n",
      "32797\n",
      "20 7151\n",
      "7152 14363\n",
      "14363 21574\n",
      "21575 28786\n",
      "Tomasz stresstest 2\n",
      "52605\n",
      "20 7152\n",
      "7153 14364\n",
      "14364 21575\n",
      "21576 28787\n",
      "28787 35998\n",
      "35999 43210\n",
      "43210 50421\n",
      "Thomas nultest 1\n",
      "32632\n",
      "20 7152\n",
      "7153 14364\n",
      "14364 21575\n",
      "21576 28787\n",
      "Thomas nultest 2\n",
      "16496\n",
      "20 7152\n",
      "7153 14364\n",
      "Thomas rest 1\n",
      "61249\n",
      "20 7155\n",
      "7156 14367\n",
      "14367 21578\n",
      "21579 28790\n",
      "28790 36001\n",
      "36002 43213\n",
      "43213 50424\n",
      "50425 57636\n",
      "Thomas rest 2\n",
      "35471\n",
      "20 7153\n",
      "7154 14365\n",
      "14365 21576\n",
      "21577 28788\n",
      "Thomas stresstest 1\n",
      "82755\n",
      "20 7157\n",
      "7158 14369\n",
      "14369 21580\n",
      "21581 28792\n",
      "28792 36003\n",
      "36004 43215\n",
      "43215 50426\n",
      "50427 57638\n",
      "57638 64850\n",
      "64850 72061\n",
      "72062 79273\n",
      "Thomas stresstest 2\n",
      "53285\n",
      "20 7149\n",
      "7150 14361\n",
      "14361 21572\n",
      "21573 28784\n",
      "28784 35995\n",
      "35996 43207\n",
      "43207 50418\n",
      "Thomas stresstest 3\n",
      "45621\n",
      "20 7151\n",
      "7152 14363\n",
      "14363 21574\n",
      "21575 28786\n",
      "28786 35997\n",
      "35998 43209\n",
      "Mack nultest 1\n",
      "31252\n",
      "20 7152\n",
      "7153 14364\n",
      "14364 21575\n",
      "21576 28787\n",
      "Mack nultest 2\n",
      "26461\n",
      "20 7153\n",
      "7154 14365\n",
      "14365 21576\n",
      "Mack rest 1\n",
      "43433\n",
      "20 7153\n",
      "7154 14365\n",
      "14365 21576\n",
      "21577 28788\n",
      "28788 35999\n",
      "36000 43211\n",
      "Mack rest 2\n",
      "37590\n",
      "20 7143\n",
      "7144 14355\n",
      "14355 21566\n",
      "21567 28778\n",
      "28778 35989\n",
      "Mack stresstest 1\n",
      "105936\n",
      "20 7153\n",
      "7154 14365\n",
      "14365 21576\n",
      "21577 28788\n",
      "28788 35999\n",
      "36000 43211\n",
      "43211 50422\n",
      "50423 57634\n",
      "57634 64846\n",
      "64846 72057\n",
      "72058 79269\n",
      "79269 86480\n",
      "86481 93692\n",
      "93692 100903\n",
      "Mack stresstest 2\n",
      "50345\n",
      "20 7154\n",
      "7155 14366\n",
      "14366 21577\n",
      "21578 28789\n",
      "28789 36000\n",
      "36001 43212\n",
      "Jack nultest 1\n",
      "37155\n",
      "20 7114\n",
      "7115 14326\n",
      "14326 21537\n",
      "21538 28749\n",
      "28749 35960\n",
      "Jack nultest 2\n",
      "26725\n",
      "20 7114\n",
      "7115 14326\n",
      "14326 21537\n",
      "Jack rest 1\n",
      "58925\n",
      "20 7118\n",
      "7119 14330\n",
      "14330 21541\n",
      "21542 28753\n",
      "28753 35964\n",
      "35965 43176\n",
      "43176 50387\n",
      "50388 57599\n",
      "Jack rest 2\n",
      "33467\n",
      "20 7119\n",
      "7120 14331\n",
      "14331 21542\n",
      "21543 28754\n",
      "Jack stresstest 1\n",
      "119055\n",
      "20 7097\n",
      "7098 14309\n",
      "14309 21520\n",
      "21521 28732\n",
      "28732 35943\n",
      "35944 43155\n",
      "43155 50366\n",
      "50367 57578\n",
      "57578 64790\n",
      "64790 72001\n",
      "72002 79213\n",
      "79213 86424\n",
      "86425 93636\n",
      "93636 100847\n",
      "100848 108058\n",
      "108059 115270\n",
      "Jack stresstest 2\n",
      "36305\n",
      "20 7125\n",
      "7126 14337\n",
      "14337 21548\n",
      "21549 28760\n",
      "28760 35971\n",
      "Richelle nultest 1\n",
      "40851\n",
      "20 7117\n",
      "7118 14329\n",
      "14329 21540\n",
      "21541 28752\n",
      "28752 35963\n",
      "Richelle nultest 2\n",
      "33719\n",
      "20 7110\n",
      "7111 14322\n",
      "14322 21533\n",
      "21534 28745\n",
      "Richelle rest 1\n",
      "60423\n",
      "20 7103\n",
      "7104 14315\n",
      "14315 21526\n",
      "21527 28738\n",
      "28738 35949\n",
      "35950 43161\n",
      "43161 50372\n",
      "50373 57584\n",
      "Richelle rest 2\n",
      "45012\n",
      "20 7120\n",
      "7121 14332\n",
      "14332 21543\n",
      "21544 28755\n",
      "28755 35966\n",
      "35967 43178\n",
      "Richelle stresstest 1\n",
      "106520\n",
      "20 7124\n",
      "7125 14336\n",
      "14336 21547\n",
      "21548 28759\n",
      "28759 35970\n",
      "35971 43182\n",
      "43182 50393\n",
      "50394 57605\n",
      "57605 64817\n",
      "64817 72028\n",
      "72029 79240\n",
      "79240 86451\n",
      "86452 93663\n",
      "93663 100874\n",
      "Richelle stresstest 2\n",
      "71256\n",
      "20 7124\n",
      "7125 14336\n",
      "14336 21547\n",
      "21548 28759\n",
      "28759 35970\n",
      "35971 43182\n",
      "43182 50393\n",
      "50394 57605\n",
      "57605 64817\n",
      "Richelle stresstest 3\n",
      "46354\n",
      "20 7110\n",
      "7111 14322\n",
      "14322 21533\n",
      "21534 28745\n",
      "28745 35956\n",
      "35957 43168\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "duration=45\n",
    "meas_subjects = ['Nicky', 'Rick', 'Tomasz', 'Thomas', 'Mack', 'Jack', 'Richelle']\n",
    "for testday in range(1,3):\n",
    "    for meas_subject in range(1,8):\n",
    "        for type_test in ['nultest', 'rest', 'stresstest']:\n",
    "            go = True\n",
    "            num_sig = 1\n",
    "            while go:\n",
    "                meas_path= r\"C:/Users/riche/OneDrive/Documenten/GitHub/EPO4-C2/Testday_\" + str(testday) + '/Subject_' + str(meas_subject)\n",
    "                meas_path_complete = meas_path + '/' + meas_subjects[meas_subject-1] + '-' + type_test + '_' + str(num_sig) + '.csv'\n",
    "                # print(meas_path_complete)\n",
    "                check_file = os.path.isfile(meas_path_complete)\n",
    "                if check_file:\n",
    "                    print(meas_subjects[meas_subject-1], type_test, num_sig)\n",
    "                    num_sig += 1\n",
    "                    meas_data = pd.read_csv(meas_path_complete)\n",
    "                    meas_ecg_base = meas_data['ECG Data']\n",
    "                    meas_eda_base = meas_data['GSR Data']\n",
    "                    meas_t = meas_data['TimeStamp']\n",
    "                    meas_bvp_base = meas_data['PPG_Red Data']\n",
    "\n",
    "                    meas_ecg_base = meas_ecg_base[20:]\n",
    "                    meas_eda_base = meas_eda_base[20:]\n",
    "                    meas_bvp_base = meas_bvp_base[20:]\n",
    "                    meas_t = meas_t[20:]\n",
    "                    print(len(meas_ecg_base))\n",
    "                    n_samples = duration * meas_fs\n",
    "                    for j in range(meas_t[len(meas_t)-1]//n_samples): # loop over all segments\n",
    "                        # Cutting the signal into a segment\n",
    "                        begin, stop = j*meas_fs*duration, (j+1)*meas_fs*duration\n",
    "                        \n",
    "                        start = meas_t.index[meas_t>=begin][0]\n",
    "                        end = meas_t.index[meas_t<=stop][-1]\n",
    "\n",
    "                        print(start, end)\n",
    "\n",
    "                        meas_ecg = meas_ecg_base[start:end]\n",
    "                        meas_eda = meas_eda_base[start:end]\n",
    "                        meas_bvp = meas_bvp_base[start:end]\n",
    "                        \n",
    "                        # getting the eda and ecg features\n",
    "                        index += 1\n",
    "\n",
    "                        tonic, phasic, start, end = get_edaindex(meas_eda, meas_fs)\n",
    "                        meas_edafeatures = get_edafeatures(index, meas_edafeatures, phasic, tonic, meas_fs)\n",
    "                        meas_ecgfeatures = get_ecgfeatures(meas_ecg, meas_fs, meas_ecgfeatures, index)\n",
    "                        \n",
    "                        bvpfeature = get_bvpfeatures(meas_bvp, index)\n",
    "                        meas_bvpfeatures = pd.concat([meas_bvpfeatures, bvpfeature], axis=0) \n",
    "\n",
    "            \n",
    "\n",
    "                        if type_test == 'stresstest':\n",
    "                            meas_y.append(1)\n",
    "                        else:\n",
    "                            meas_y.append(0)\n",
    "                else:\n",
    "                    go = False\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_bvp_filt = bvp_prep(meas_bvpfeatures)\n",
    "\n",
    "meas_features1 = pd.merge(meas_ecgfeatures, meas_edafeatures, left_index=True, right_index=True)\n",
    "meas_features = pd.merge(meas_bvp_filt, meas_features1, on='index')\n",
    "meas_features['y'] = meas_y\n",
    "# meas_features = meas_features[meas_features.RSP_RVT != 0.0]\n",
    "# meas_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_features = meas_features.fillna(method=\"ffill\")\n",
    "meas_features = meas_features.fillna(method=\"bfill\")\n",
    "meas_features = meas_features.dropna(axis='columns')\n",
    "mfeatures = meas_features\n",
    "mfeatures.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "mfeatures = mfeatures.dropna(axis='columns')\n",
    "meas_features = mfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_features.to_csv(\"meas_features.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which gives predicted and correct values with missclassifications and scores for rest models\n",
    "def model_predict(X_test, y_test1, model):\n",
    "    predictions = model.predict(X_test)\n",
    "    miss_class = np.where(predictions != y_test1)\n",
    "    miss_class = miss_class[0]\n",
    "    score=model.score(X_test, y_test1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_predict_neural(X_test, y_test1, model):\n",
    "#     predictions = model.predict(X_test)\n",
    "#     predictions = np.argmax(predictions, axis = 1)\n",
    "#     y_test1 = y_test1.astype('int64')\n",
    "#     testYarg = np.argmax(y_test1, axis = 1)\n",
    "#     miss_class = np.where(predictions != testYarg)\n",
    "#     miss_class = miss_class[0]\n",
    "#     print(\"Neural network\")\n",
    "#     print(\"Predicted:\",predictions)\n",
    "#     print(\"Correct:  \",testYarg)\n",
    "#     print(\"Index missclassified:\", miss_class)\n",
    "#     # print(\"Score:\", val_accuracy, \"\\n\")\n",
    "#     return predictions, miss_class, testYarg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_test_cat \u001b[39m=\u001b[39m to_categorical(y_test)\n\u001b[0;32m      2\u001b[0m y_test1 \u001b[39m=\u001b[39m y_test_cat\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m testYarg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_test1, axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# y_test_cat = to_categorical(y_test)\n",
    "# y_test1 = y_test_cat.astype('int64')\n",
    "# testYarg = np.argmax(y_test1, axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLogisticRegression()\tSVC()\tRandomForestClassifier()\tAdaBoostClassifier(learning_rate=1, random_state=5)\tSequential()\tLabelEncoder()\n",
      "variance threshold\t0.48594+-0.010\t0.48047+-0.000\t0.55911+-0.025\t0.55312+-0.027\t0.48177+-0.005\t0.55469+-0.035\n",
      "pearson correlation\t0.49870+-0.020\t0.51953+-0.000\t0.57943+-0.020\t0.56198+-0.035\t0.47969+-0.003\t0.55833+-0.041\n",
      "chi-squared\t0.48047+-0.000\t0.51953+-0.000\t0.57708+-0.012\t0.57318+-0.016\t0.48047+-0.000\t0.57734+-0.008\n",
      "forward wrapper\t0.58828+-0.007\t0.51068+-0.015\t0.58021+-0.009\t0.58203+-0.027\t0.55729+-0.020\t0.62604+-0.031\n",
      "backwards wrapper\t0.49375+-0.003\t0.48594+-0.004\t0.52995+-0.023\t0.54271+-0.023\t0.49505+-0.027\t0.51875+-0.024\n",
      "step-wise wrapper\t0.49479+-0.022\t0.48047+-0.000\t0.57031+-0.011\t0.55104+-0.032\t0.48620+-0.018\t0.58099+-0.018\n",
      "Lasso\t0.48047+-0.000\t0.51953+-0.000\t0.58802+-0.009\t0.58177+-0.025\t0.48047+-0.000\t0.56068+-0.030\n",
      "Random Forest\t0.52969+-0.004\t0.48047+-0.000\t0.54505+-0.033\t0.56016+-0.025\t0.48958+-0.025\t0.57083+-0.028\n",
      "lightGBM\t0.53203+-0.008\t0.55443+-0.025\t0.57969+-0.010\t0.57474+-0.039\t0.55703+-0.031\t0.54505+-0.037\n"
     ]
    }
   ],
   "source": [
    "subjects = ['S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S13','S14','S15','S16','S17']\n",
    "model_names = [\"LogisticRegression()\", \"SVC()\", \"RandomForestClassifier()\", \"AdaBoostClassifier(learning_rate=1, random_state=5)\", \"Sequential()\",\n",
    "               \"LabelEncoder()\"]\n",
    "\n",
    "first_line = \"\"\n",
    "for model in model_names:\n",
    "    first_line += \"\\t\" + model\n",
    "print(first_line)\n",
    "\n",
    "for i in range(len(selected_features)):\n",
    "    feat = selected_features[i]\n",
    "    scores_dict = dict()\n",
    "    for model in model_names:\n",
    "        scores_dict[model] = []\n",
    "\n",
    "    for subject in subjects:\n",
    "        # X_test = total_features[total_features['index'].str.startswith(subject)]\n",
    "        # y_test = X_test['y']\n",
    "        # X_test = X_test.drop(['y', 'index'], axis=1)\n",
    "        X_train = total_features[~total_features['index'].str.startswith(subject)]\n",
    "        y_train = X_train['y']\n",
    "        X_train = X_train.drop(['y', 'index'], axis=1)\n",
    "        # X_train = X_train[list(feat)]\n",
    "        # X_test = X_test[list(feat)]\n",
    "\n",
    "        # External testing\n",
    "        X_test = meas_features\n",
    "        y_test = X_test['y']\n",
    "        feat_lst = []\n",
    "        for feature in feat:\n",
    "            if feature in X_test.keys():\n",
    "                feat_lst.append(feature)\n",
    "        # print(feat_lst)\n",
    "        X_test = meas_features.drop(['y', 'index'], axis=1)\n",
    "        X_train = X_train[list(feat_lst)]\n",
    "        X_test = X_test[list(feat_lst)]\n",
    "        \n",
    "        # fit the data\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Get models\n",
    "        lg_model = logistic_func(X_train, X_test, y_train, y_test)\n",
    "        svm_model = svm_func(X_train, X_test, y_train, y_test)\n",
    "        rf_model = randomforest_func(X_train, X_test, y_train, y_test)\n",
    "        ada_model = adaboost_func(X_train, X_test, y_train, y_test)\n",
    "        nn_accurary = neuralnetworks_func(X_train, X_test, y_train, y_test)\n",
    "        xgb_accuracy = xgboost_func(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Get the score\n",
    "        models= [lg_model, svm_model, rf_model, ada_model]\n",
    "\n",
    "        for model in models:\n",
    "            score = model_predict(X_test, y_test, model)\n",
    "            scores_dict[str(model)].append(model_predict(X_test, y_test, model))\n",
    "        scores_dict[\"Sequential()\"].append(nn_accurary)\n",
    "        scores_dict[\"LabelEncoder()\"].append(xgb_accuracy)\n",
    "\n",
    "    line = str(feature_sel_name[i])\n",
    "\n",
    "    for a in scores_dict:\n",
    "        score = scores_dict[a]\n",
    "        line += \"\\t\" + f\"{np.mean(score):.5f}\" + \"+-\" + f\"{np.std(score):.3f}\"\n",
    "\n",
    "    # line += \"\\t\" + f\"{np.mean(nn_accurary):.5f}\" + \"+-\" + f\"{np.std(nn_accurary):.3f}\"\n",
    "    print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject-based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0.95497 +- 0.016\t 0.94966 +- 0.020\t 0.95233 +- 0.020\t 0.94695 +- 0.018\t 0.95674 +- 0.022\t 0.95411 +- 0.020\n",
      "\t 0.94609 +- 0.018\t 0.96289 +- 0.019\t 0.94697 +- 0.014\t 0.95053 +- 0.021\t 0.95763 +- 0.017\t 0.94522 +- 0.019\n",
      "\t 0.96290 +- 0.017\t 0.96207 +- 0.020\t 0.95230 +- 0.020\t 0.96022 +- 0.011\t 0.96287 +- 0.017\t 0.97081 +- 0.012\n",
      "\t 0.93732 +- 0.031\t 0.93820 +- 0.021\t 0.93285 +- 0.023\t 0.90981 +- 0.030\t 0.90991 +- 0.029\t 0.92399 +- 0.024\n",
      "\t 0.87273 +- 0.029\t 0.87986 +- 0.027\t 0.87632 +- 0.033\t 0.88243 +- 0.020\t 0.86041 +- 0.041\t 0.87367 +- 0.034\n",
      "\t 0.94167 +- 0.019\t 0.95058 +- 0.021\t 0.95324 +- 0.025\t 0.93553 +- 0.024\t 0.94529 +- 0.025\t 0.94697 +- 0.018\n",
      "\t 0.97793 +- 0.012\t 0.95319 +- 0.022\t 0.94439 +- 0.022\t 0.94432 +- 0.023\t 0.97000 +- 0.019\t 0.93725 +- 0.016\n",
      "\t 0.92842 +- 0.020\t 0.93462 +- 0.021\t 0.96201 +- 0.016\t 0.95228 +- 0.022\t 0.95323 +- 0.022\t 0.96025 +- 0.022\n",
      "\t 0.76574 +- 0.032\t 0.79228 +- 0.032\t 0.79849 +- 0.033\t 0.81173 +- 0.030\t 0.80464 +- 0.027\t 0.76207 +- 0.031\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(selected_features)):\n",
    "    feat = selected_features[i]\n",
    "    sb_dict = dict()\n",
    "\n",
    "    for model in model_names:\n",
    "        sb_dict[model] = []\n",
    "        \n",
    "    for subject in subjects:\n",
    "        X_sb = total_features[~total_features['index'].str.startswith(subject)]\n",
    "        y_sb = X_sb['y']\n",
    "        X_sb = X_sb.drop(['y', 'index'], axis=1)\n",
    "        X_sb = X_sb[list(feat)]\n",
    "\n",
    "        X_train_sb, X_test_sb, y_train_sb, y_test_sb = train_test_split(X_sb, y_sb, test_size=0.2, random_state=5)\n",
    "\n",
    "        scaler_sb = StandardScaler().fit(X_train_sb)\n",
    "        X_train_sb = scaler_sb.transform(X_train_sb)\n",
    "        X_test_sb = scaler_sb.transform(X_test_sb)\n",
    "\n",
    "        lg_model_sb = logistic_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "        svm_model_sb = svm_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "        rf_model_sb = randomforest_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "        # nn_model = neuralnetworks_func(X_train, X_test, Y_train, Y_test)\n",
    "        ada_model_sb = adaboost_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "        nn_accurary_sb = neuralnetworks_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "        xgb_accuracy_sb = xgboost_func(X_train_sb, X_test_sb, y_train_sb, y_test_sb)\n",
    "        # Get the score\n",
    "        models= [lg_model_sb, svm_model_sb, rf_model_sb, ada_model_sb]\n",
    "\n",
    "        for model in models:\n",
    "            score = model_predict(X_test_sb, y_test_sb, model)\n",
    "            sb_dict[str(model)].append(model_predict(X_test_sb, y_test_sb, model))\n",
    "        sb_dict[\"Sequential()\"].append(nn_accurary_sb)\n",
    "        sb_dict[\"LabelEncoder()\"].append(xgb_accuracy_sb)\n",
    "\n",
    "        # for model in models:\n",
    "        #     sb_dict[str(model)].append(model_predict(X_test_sb, y_test_sb, model))\n",
    "\n",
    "    line = \"\"\n",
    "    for a in sb_dict:\n",
    "        line += f\"\\t {np.mean(sb_dict[a]):.5f}+-{np.std(sb_dict[a]):.3f}\"\n",
    "    print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLogisticRegression()\tSVC()\tRandomForestClassifier()\tAdaBoostClassifier(learning_rate=1, random_state=5)\tSequential()\tLabelEncoder()\n",
      "variance threshold\t0.96994+-0.074\t0.97701+-0.064\t0.97361+-0.050\t0.96390+-0.054\t0.97454+-0.064\t0.96351+-0.064\n",
      "pearson correlation\t0.96994+-0.074\t0.97701+-0.064\t0.96864+-0.060\t0.96390+-0.054\t0.96921+-0.069\t0.96351+-0.064\n",
      "chi-squared\t0.96994+-0.074\t0.97701+-0.064\t0.97114+-0.050\t0.96390+-0.054\t0.95984+-0.073\t0.96351+-0.064\n",
      "forward wrapper\t0.96994+-0.074\t0.97701+-0.064\t0.97361+-0.054\t0.96390+-0.054\t0.97207+-0.065\t0.96351+-0.064\n",
      "backwards wrapper\t0.96994+-0.074\t0.97701+-0.064\t0.97114+-0.050\t0.96390+-0.054\t0.96251+-0.088\t0.96351+-0.064\n",
      "step-wise wrapper\t0.96994+-0.074\t0.97701+-0.064\t0.97094+-0.054\t0.96390+-0.054\t0.97168+-0.069\t0.96351+-0.064\n",
      "Lasso\t0.96994+-0.074\t0.97701+-0.064\t0.97094+-0.054\t0.96390+-0.054\t0.96728+-0.079\t0.96351+-0.064\n",
      "Random Forest\t0.96994+-0.074\t0.97701+-0.064\t0.97361+-0.050\t0.96390+-0.054\t0.96444+-0.078\t0.96351+-0.064\n",
      "lightGBM\t0.96994+-0.074\t0.97701+-0.064\t0.97131+-0.057\t0.96390+-0.054\t0.96654+-0.070\t0.96351+-0.064\n"
     ]
    }
   ],
   "source": [
    "subjects = ['S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S13','S14','S15','S16','S17']\n",
    "model_names = [\"LogisticRegression()\", \"SVC()\", \"RandomForestClassifier()\", \"AdaBoostClassifier(learning_rate=1, random_state=5)\", \"Sequential()\",\n",
    "               \"LabelEncoder()\"]\n",
    "\n",
    "first_line = \"\"\n",
    "for model in model_names:\n",
    "    first_line += \"\\t\" + model\n",
    "print(first_line)\n",
    "\n",
    "y_hy_list= []\n",
    "amount_per_subject = []\n",
    "x_trains = np.empty((len(total_features), len(feat)))\n",
    "total = 0\n",
    "for subject in subjects:\n",
    "    X_hy = total_features[total_features['index'].str.startswith(subject)]\n",
    "    y_hy_list.extend(list(X_hy['y']))\n",
    "    X_hy = X_hy.drop(['y', 'index'], axis=1)\n",
    "    X_hy = X_hy[list(feat)]\n",
    "    scaler_hy = StandardScaler().fit(X_hy)\n",
    "    x_trains[total:total+len(X_hy),:] = (scaler_hy.transform(X_hy))\n",
    "    amount_per_subject.append(len(X_hy))\n",
    "    total+= len(X_hy)\n",
    "\n",
    "for i in range(len(selected_features)):\n",
    "    feat = selected_features[i]\n",
    "    scores_dict = dict()\n",
    "\n",
    "    for model in model_names:\n",
    "        scores_dict[model] = []\n",
    "\n",
    "    start = 0\n",
    "    for subject in range(len(subjects)):\n",
    "        X_test = x_trains[start:start+amount_per_subject[subject]]\n",
    "        y_test = y_hy_list[start:start+amount_per_subject[subject]]\n",
    "        y_train = []\n",
    "        if start==0:\n",
    "            X_train = x_trains[start+amount_per_subject[subject]:]\n",
    "            y_train = y_hy_list[start+amount_per_subject[subject]:]\n",
    "        elif start==len(subjects)-1:\n",
    "            X_train = x_trains[:start]\n",
    "            y_train = y_hy_list[:start]\n",
    "        else:\n",
    "            X_train = np.concatenate((x_trains[:start],x_trains[start + amount_per_subject[subject]:]))\n",
    "            y_train = y_hy_list[:start] + y_hy_list[start + amount_per_subject[subject]:]\n",
    "        \n",
    "        start += amount_per_subject[subject]\n",
    "     \n",
    "        # Get models\n",
    "        lg_model = logistic_func(X_train, X_test, y_train, y_test)\n",
    "        svm_model = svm_func(X_train, X_test, y_train, y_test)\n",
    "        rf_model = randomforest_func(X_train, X_test, y_train, y_test)\n",
    "        ada_model = adaboost_func(X_train, X_test, y_train, y_test)\n",
    "        nn_accurary = neuralnetworks_func(X_train, X_test, y_train, y_test)\n",
    "        xgb_accuracy = xgboost_func(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Get the score\n",
    "        models= [lg_model, svm_model, rf_model, ada_model]\n",
    "\n",
    "        for model in models:\n",
    "            score = model_predict(X_test, y_test, model)\n",
    "            scores_dict[str(model)].append(model_predict(X_test, y_test, model))\n",
    "        scores_dict[\"Sequential()\"].append(nn_accurary)\n",
    "        scores_dict[\"LabelEncoder()\"].append(xgb_accuracy)\n",
    "\n",
    "    line = str(feature_sel_name[i])\n",
    "\n",
    "    for a in scores_dict:\n",
    "        score = scores_dict[a]\n",
    "        line += \"\\t\" + f\"{np.mean(score):.5f}\" + \"+-\" + f\"{np.std(score):.3f}\"\n",
    "\n",
    "    # line += \"\\t\" + f\"{np.mean(nn_accurary):.5f}\" + \"+-\" + f\"{np.std(nn_accurary):.3f}\"\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
