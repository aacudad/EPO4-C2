{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\antropy\\fractal.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit('float64(float64[:], int32)')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import neurokit2 as nk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pyhrv.time_domain as td\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pyhrv\n",
    "import ipynb\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# functions for features extraction\n",
    "from ipynb.fs.full.ECG_features import get_ecgfeatures\n",
    "from ipynb.fs.full.EDA import get_edaindex, get_edafeatures\n",
    "from ipynb.fs.full.BVP import get_bvpfeatures, bvp_prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection & Feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset reading\n",
    "class read_data_of_one_subject:\n",
    "            \"\"\"Read data from WESAD dataset\"\"\"\n",
    "            def __init__(self, path, subject):\n",
    "                self.keys = ['label', 'subject', 'signal']\n",
    "                self.signal_keys = ['wrist', 'chest']\n",
    "                self.chest_sensor_keys = ['ACC', 'ECG', 'EDA', 'EMG', 'Resp', 'Temp']\n",
    "                self.wrist_sensor_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "                #os.chdir(path)\n",
    "                #os.chdir(subject)\n",
    "                with open(path + subject +'/'+subject + '.pkl', 'rb') as file:\n",
    "                    data = pickle.load(file, encoding='latin1')\n",
    "                self.data = data\n",
    "\n",
    "            def get_labels(self):\n",
    "                return self.data[self.keys[0]]\n",
    "\n",
    "            def get_wrist_data(self):\n",
    "                \"\"\"\"\"\"\n",
    "                #label = self.data[self.keys[0]]\n",
    "                #assert subject == self.data[self.keys[1]]\n",
    "                signal = self.data[self.keys[2]]\n",
    "                wrist_data = signal[self.signal_keys[0]]\n",
    "                #wrist_ACC = wrist_data[self.wrist_sensor_keys[0]]\n",
    "                #wrist_ECG = wrist_data[self.wrist_sensor_keys[1]]\n",
    "                return wrist_data\n",
    "\n",
    "            def get_chest_data(self):\n",
    "                \"\"\"\"\"\"\n",
    "                signal = self.data[self.keys[2]]\n",
    "                chest_data = signal[self.signal_keys[1]]\n",
    "                return chest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "subject: S2\n",
      "total samples S2: 29\n",
      "subject: S3\n",
      "total samples S3: 29\n",
      "subject: S4\n",
      "total samples S4: 29\n",
      "subject: S5\n",
      "total samples S5: 29\n",
      "subject: S6\n",
      "total samples S6: 29\n",
      "subject: S7\n",
      "total samples S7: 29\n",
      "subject: S8\n",
      "total samples S8: 29\n",
      "subject: S9\n",
      "total samples S9: 29\n",
      "subject: S10\n",
      "total samples S10: 30\n",
      "subject: S11\n",
      "total samples S11: 30\n",
      "subject: S12\n",
      "total samples S12: 0\n",
      "subject: S13\n",
      "total samples S13: 29\n",
      "subject: S14\n",
      "total samples S14: 30\n",
      "subject: S15\n",
      "total samples S15: 30\n",
      "subject: S16\n",
      "total samples S16: 29\n",
      "subject: S17\n",
      "total samples S17: 29\n"
     ]
    }
   ],
   "source": [
    "# Set up empty dataframes for the features\n",
    "columns_ecg =['index', 'ecg_HR_mean', 'ecg_HR_min', 'ecg_HR_max', 'ecg_HR_std', 'ecg_SDNN', 'ecg_SDANN', 'ecg_RMSSD', \n",
    "                                           'ecg_SDSD','ecg_pNN50', 'ecg_pNN20', \"ecg_triangular_index\", \"ecg_tinn\", \"ecg_sd1\", \"ecg_sd2\",\n",
    "                                            \"ecg_ratio_sd2_sd1\", 'ecg_abs_power_VLF', 'ecg_abs_power_LF', 'ecg_abs_power_HF', 'ecg_tot_power',\n",
    "                                            'ecg_LF/HF', 'ecg_peak_vlf', 'ecg_peak_lf', 'ecg_peak_hf', 'ecg_norm_power_LF', 'ecg_norm_power_HF',\n",
    "                                            'max_cwt_absmean', 'min_cwt_absmean', 'mean_cwt_absmean','std_cwt_absmean', \n",
    "                                            'max_cwt_std', 'min_cwt_std', 'mean_cwt_std','std_cwt_std',\n",
    "                                            'max_cwt_energy', 'min_cwt_energy', 'mean_cwt_energy','std_cwt_energy',\n",
    "                                            'max_cwt_var', 'min_cwt_var', 'mean_cwt_var','std_cwt_var',\n",
    "                                            'entropy']\n",
    "columns_resp = ['RSP_Rate_Mean','RRV_RMSSD', 'RRV_MeanBB', 'RRV_SDBB', 'RRV_SDSD', 'RRV_CVBB', 'RRV_CVSD', 'RRV_MedianBB',\n",
    "                'RRV_MadBB', 'RRV_MCVBB', 'RRV_VLF', 'RRV_LF', 'RRV_HF', 'RRV_LFHF', 'RRV_LFn', 'RRV_HFn', 'RRV_SD1',\n",
    "                'RRV_SD2', 'RRV_SD2SD1', 'RRV_ApEn', 'RRV_SampEn', 'RSP_Amplitude_Mean', 'RSP_RVT', 'RSP_Symmetry_PeakTrough',\n",
    "                'RSP_Symmetry_RiseDecay', 'RSP_Phase_Duration_Inspiration', 'RSP_Phase_Duration_Expiration', 'RSP_Phase_Duration_Ratio']\n",
    "\n",
    "columns = [*columns_ecg, *columns_resp]\n",
    "ecgfeatures = pd.DataFrame(None, columns= columns)\n",
    "edafeatures = None\n",
    "bvpfeatures = None\n",
    "\n",
    "print(len(ecgfeatures.keys()))\n",
    "total_samples = 0\n",
    "fs = 700 # sampling frequency\n",
    "fs_bvp = 64\n",
    "duration = 45\n",
    "y = []\n",
    "amountpsample = dict()\n",
    "# data_set_path= \"C:/Users/JackC/Documents/EPO4/WESAD/WESAD/\" # Folder path\n",
    "data_set_path= \"C:/Users/riche/Downloads/WESAD/WESAD/\" # Folder path\n",
    "# data_set_path = \"C:/Users/Adnane/Downloads/WESAD/WESAD/\"\n",
    "\n",
    "for i in range(16): # 15 subjects\n",
    "    subject = 'S'+str(i+2) # Cycle through S2 to S17\n",
    "    print(f\"subject: {subject}\")\n",
    "    amountpsample[subject] = 0\n",
    "    if subject != 'S12': # Skip S12, because it does not exist\n",
    "        # Object instantiation\n",
    "        obj_data = {}\n",
    "\n",
    "        # Accessing class attributes and method through objects\n",
    "        obj_data[subject] = read_data_of_one_subject(data_set_path, subject)\n",
    "\n",
    "        chest_data_dict = obj_data[subject].get_chest_data()\n",
    "        wrist_data_dict = obj_data[subject].get_wrist_data()\n",
    "        chest_dict_length = {key: len(value) for key, value in chest_data_dict.items()}\n",
    "        wrist_dict_length = {key: len(value) for key, value in wrist_data_dict.items()}\n",
    "\n",
    "        # Get labels\n",
    "        labels = obj_data[subject].get_labels()\n",
    "\n",
    "        for label in [1,2,4]: # for every state of stress\n",
    "            \n",
    "            baseline = np.asarray([idx for idx,val in enumerate(labels) if val == label])\n",
    "\n",
    "            # Obtaining the chest data\n",
    "            eda_base=chest_data_dict['EDA'][baseline,0] # Select the EDA data\n",
    "            ecg_base=chest_data_dict['ECG'][baseline,0] # Select the ECG data\n",
    "            resp_base=chest_data_dict['Resp'][baseline,0] # Select the respiration data\n",
    "\n",
    "            # Obtaining the wrist data\n",
    "            baseline_BVP = baseline * fs_bvp // fs\n",
    "            bvp_base=wrist_data_dict['BVP'][baseline_BVP,0] # Select the BVP data\n",
    "\n",
    "            # Multiple smaller samples per signal\n",
    "            # samples = 10 # specify amount of samples\n",
    "\n",
    "            for j in range(0, len(eda_base)//(duration*fs), 2): # loop over all segments\n",
    "                amountpsample[subject] += 1\n",
    "                # adding the labels to y\n",
    "                if label==1 or label==4: # if amusement or medidation, classify as non-stress\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                # Cutting the signal into a segment\n",
    "                start, end = j*fs*duration, (j+1)*fs*duration\n",
    "                ecg = ecg_base[start: end]\n",
    "                eda = eda_base[start: end]\n",
    "                bvp = bvp_base[start: end]\n",
    "\n",
    "                # splitting sample into 10 smaller even-sized parts\n",
    "                #ecg = ecg_base[int(len(ecg_base)* (j/samples)) : int(len(ecg_base)* ((j+1)/samples))]\n",
    "                #eda = eda_base[int(len(eda_base)* (j/samples)) : int(len(eda_base)* ((j+1)/samples))]\n",
    "\n",
    "                # getting the eda and ecg features\n",
    "                index = subject + str(label) + str(j)\n",
    "                tonic, phasic, start, end = get_edaindex(eda, fs)\n",
    "                edafeatures = get_edafeatures(index, edafeatures, phasic, tonic, fs)\n",
    "                ecgfeatures = get_ecgfeatures(ecg, fs, ecgfeatures, index)\n",
    "                \n",
    "                bvpfeature = get_bvpfeatures(bvp, fs_bvp, index)\n",
    "                bvpfeatures = pd.concat([bvpfeatures, bvpfeature], axis=0)\n",
    "    print(f\"total samples {subject}: {amountpsample[subject]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_edafeatures = edafeatures\n",
    "# saved_ecgfeatures = ecgfeatures\n",
    "# saved_bvpfeatures = bvpfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riche\\OneDrive\\Documenten\\GitHub\\EPO4-C2\\BVP.ipynb:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"    features_event.reset_index(drop=True, inplace=True)\\n\",\n"
     ]
    }
   ],
   "source": [
    "bvp_filt = bvp_prep(bvpfeatures) # remove nan and inf in BVP features\n",
    "# ecgfeatures = bvp_prep(ecgfeatures)\n",
    "\n",
    "# saving the feature data in csv files\n",
    "edafeatures.to_csv(\"EDA_features.csv\")\n",
    "ecgfeatures.to_csv(\"ECG_features.csv\")\n",
    "bvp_filt.to_csv(\"BVP_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #reading from csv file if necessary\n",
    "# edafeatures = pd.read_csv(\"EDA_features.csv\", index_col=0)\n",
    "# ecgfeatures = pd.read_csv(\"ECG_features.csv\", index_col=0)\n",
    "# bvp_filt = pd.read_csv(\"BVP_features.csv\", index_col=0)\n",
    "#features = pd.read_csv(\"features_\"+str(samples)+\"_samp.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = pd.merge(ecgfeatures, edafeatures, left_index=True, right_index=True)\n",
    "total_features = pd.merge(bvp_filt, features1, on='index')\n",
    "total_features['y'] = y\n",
    "\n",
    "total_features = total_features[total_features.RSP_RVT != 0.0]\n",
    "total_features.to_csv(\"out_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_total_features = total_features\n",
    "# total_features = pd.read_csv(\"out_features.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riche\\OneDrive\\Documenten\\GitHub\\EPO4-C2\\BVP.ipynb:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"    features_event.reset_index(drop=True, inplace=True)\\n\",\n"
     ]
    }
   ],
   "source": [
    "total_features = bvp_prep(total_features)\n",
    "features = total_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402 402\n"
     ]
    }
   ],
   "source": [
    "# Scaling the data\n",
    "# scaler = StandardScaler().fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "num_features = 25\n",
    "y = total_features['y']\n",
    "X_total = total_features.drop(['y', 'index'], axis=1)\n",
    "print(len(y), len(X_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Threshold\n",
    "scaler = MinMaxScaler().fit(X_total)\n",
    "X_train = scaler.transform(X_total)\n",
    "\n",
    "total_features_index = []\n",
    "for feat in X_total:\n",
    "    total_features_index.append(feat)\n",
    "\n",
    "X_vt = X_train[0:,0:len(X_total.keys())]\n",
    "\n",
    "v_threshold = VarianceThreshold(threshold=0.005) # Set a threshold\n",
    "v_threshold.fit(X_vt)\n",
    "index = v_threshold.get_support()\n",
    "true_index = [i for i, x in enumerate(index) if x]\n",
    "vt_features = [total_features_index[i] for i in true_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pearson correlation\n",
    "def cor_selector(X_cor, y,num_feats):\n",
    "    cor_list = []\n",
    "    feature_name = X_cor.columns.tolist()\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X_cor.columns.tolist():\n",
    "        cor = np.corrcoef(X_cor[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X_cor.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    return cor_support, cor_feature\n",
    "\n",
    "def pears(X):\n",
    "    cor_support, cor_feature = cor_selector(X, y, num_features)\n",
    "    return cor_feature\n",
    "\n",
    "pc_features = pears(X_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi-squared\n",
    "X_norm = MinMaxScaler().fit_transform(X_total)\n",
    "\n",
    "chi_selector = SelectKBest(chi2, k=num_features)\n",
    "chi_selector.fit(X_norm, y)\n",
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X_total.loc[:,chi_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper Forwards\n",
    "sfs = SFS(svm.SVC(),\n",
    "          k_features=num_features,\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring = 'r2',\n",
    "          cv = 0)\n",
    "\n",
    "sfs.fit(X_total, y)\n",
    "fwrapper_feature = list(sfs.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper Backwards\n",
    "sbs = SFS(svm.SVC(),\n",
    "         k_features=num_features,\n",
    "         forward=False,\n",
    "         floating=False,\n",
    "         cv=0)\n",
    "sbs.fit(X_total, y)\n",
    "bwrapper_feature = list(sbs.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-wise wrapper\n",
    "sffs = SFS(svm.SVC(),\n",
    "         k_features=num_features,\n",
    "         forward=True,\n",
    "         floating=True,\n",
    "         cv=0)\n",
    "sffs.fit(X_total, y)\n",
    "swrapper_feature = list(sffs.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO\n",
    "num_feats = num_features\n",
    "scaler = StandardScaler().fit(X_total)\n",
    "X_train = scaler.transform(X_total)\n",
    "embeded_lr_selector = SelectFromModel(LogisticRegression(C=1, penalty=\"l1\", solver='liblinear'), max_features=num_feats)\n",
    "embeded_lr_selector.fit(scaler.transform(X_total), y)\n",
    "\n",
    "embeded_lr_support = embeded_lr_selector.get_support()\n",
    "embeded_lr_feature = X_total.loc[:,embeded_lr_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "scaler = StandardScaler().fit(X_total)\n",
    "X_train = scaler.transform(X_total)\n",
    "\n",
    "embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_features)\n",
    "embeded_rf_selector.fit(X_total, y)\n",
    "\n",
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = X_total.loc[:,embeded_rf_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "\n",
    "embeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats)\n",
    "embeded_lgb_selector.fit(X_total, y)\n",
    "\n",
    "embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "embeded_lgb_feature = X_total.loc[:,embeded_lgb_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 176\n",
      "Filter:\n",
      "\t variance threshold: 150\n",
      "\t pearson correlation: 25\n",
      "\t chi-squared: 25\n",
      "Wrapper\n",
      "\t forward wrapper: 25\n",
      "\t backwards wrapper: 25\n",
      "\t step-wise wrapper: 25\n",
      "Embedded\n",
      "\t Lasso: 25\n",
      "\t Random Forest: 25\n",
      "\t lightGBM: 25\n"
     ]
    }
   ],
   "source": [
    "# Lists of features and printing the amount of features selected\n",
    "selected_features = [vt_features, pc_features, chi_feature, fwrapper_feature, bwrapper_feature, swrapper_feature, \n",
    "                    embeded_lr_feature, embeded_rf_feature, embeded_lgb_feature]\n",
    "feature_sel_name = [\"variance threshold\", \"pearson correlation\", \"chi-squared\", \"forward wrapper\", \"backwards wrapper\", \n",
    "                  \"step-wise wrapper\", \"Lasso\", \"Random Forest\", 'lightGBM']\n",
    "\n",
    "print(f\"total features: {len(total_features.keys())}\")\n",
    "print(\"Filter:\")\n",
    "for i in range(len(selected_features)):\n",
    "    print(f\"\\t {feature_sel_name[i]}: {len(selected_features[i])}\")\n",
    "    if feature_sel_name[i] == \"chi-squared\":\n",
    "        print(\"Wrapper\")\n",
    "    if feature_sel_name[i] == \"step-wise wrapper\":\n",
    "        print(\"Embedded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_func(X_train, X_test, y_train, y_test):\n",
    "    # define the model\n",
    "    logistic_model = LogisticRegression(solver='lbfgs', max_iter=100)\n",
    "\n",
    "    # fit/train the model on all features\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "    # Y_test_pred = logistic_model.predict(X_test)\n",
    "\n",
    "    # # score\n",
    "    # log_score = logistic_model.score(X_test,y_test)\n",
    "    return logistic_model #, Y_test_pred, log_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_func(X_train, X_test, y_train, y_test):\n",
    "    # define the model\n",
    "    svm_model = svm.SVC()\n",
    "\n",
    "    # fit/train the model on all features\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    # Y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "    # # score\n",
    "    # svm_score = svm_model.score(X_test,y_test)\n",
    "    return svm_model #, Y_test_pred, svm_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest_func(X_train, X_test, y_train, y_test):\n",
    "    # define the model\n",
    "    randomf_model = RandomForestClassifier()\n",
    "\n",
    "    # fit/train the model on all features\n",
    "    randomf_model.fit(X_train, y_train)\n",
    "    # Y_test_pred = randomf_model.predict(X_test)\n",
    "\n",
    "    # # score\n",
    "    # rf_score=randomf_model.score(X_test, y_test)\n",
    "    return randomf_model #, Y_test_pred, rf_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralnetworks_func(X_train, X_test, Y_train, Y_test):\n",
    "    val_accuracies = []\n",
    "    for i in range(5): # run 5 times with 5 random states to determine model accuracy\n",
    "        # Splitting the data in train and test data\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state= i + 1)\n",
    "\n",
    "        # Scaling the data\n",
    "        scaler = StandardScaler().fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        ## Convert labels to categorical\n",
    "        y_train_cat = to_categorical(y_train)\n",
    "        y_test_cat = to_categorical(y_test)\n",
    "\n",
    "        # Define the feedforward neural network\n",
    "        neural_model = Sequential()\n",
    "        neural_model.add(Dense(64, activation='relu', input_shape=(25,)))\n",
    "        neural_model.add(Dense(64, activation='relu'))\n",
    "        neural_model.add(Dense(64, activation='relu'))\n",
    "        neural_model.add(Dense(64, activation = 'relu'))\n",
    "        neural_model.add(Dropout(0.5))\n",
    "        neural_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "        # Compile the model\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        neural_model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        # history = model.fit(X_train_pca, y_train, validation_data=(X_test_pca, y_test), epochs=45, batch_size=100, verbose=2)\n",
    "        history = neural_model.fit(x_train, y_train_cat, validation_data=(x_test, y_test_cat), epochs=45, batch_size=100, verbose=0)\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "        val_accuracies.append(val_accuracy) # accuracy score\n",
    "\n",
    "    # print(val_accuracies)\n",
    "    avg_val_accuracy = sum(val_accuracies) / len(val_accuracies) # average of accurracy scores\n",
    "    # print(\"Average Validation Accuracy:\", avg_val_accuracy)\n",
    "\n",
    "    # plt.plot(history.history['accuracy'])\n",
    "    # plt.plot(history.history['val_accuracy'])\n",
    "    # plt.legend(['accuracy', 'val_accuracy'])\n",
    "    # print(neural_model.predict(X_test))\n",
    "    return neural_model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AdaBoost classifier\n",
    "def adaboost_func(X_train, X_test, Y_train, Y_test):\n",
    "\n",
    "        # Create adaboost classifer object\n",
    "    abc= AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=5)\n",
    "\n",
    "    # Train Adaboost Classifer\n",
    "    adaboost_model = abc.fit(X_train, y_train)\n",
    "\n",
    "    return adaboost_model\n",
    "    #Predict the response for test dataset\n",
    "    # y_pred = model1.predict(X_test)\n",
    "        # score30= adaboost_model.score(X_test, y_test)\n",
    "        # Ada_scores[i] = score30\n",
    "    # print(Ada_scores)\n",
    "    # print(np.mean(Ada_scores))\n",
    "    #predictions = adaboost_model.predict(X_test)\n",
    "    #print(predictions != y_test)\n",
    "    #print(score30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ada = []\n",
    "trainscores_ada = []\n",
    "for i in range (50):\n",
    "      dtree = DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=1)\n",
    "      adbclassifier = AdaBoostClassifier(base_estimator=dtree,\n",
    "                                    n_estimators=100,\n",
    "                                    learning_rate=1,\n",
    "                                    algorithm = 'SAMME',\n",
    "                                    random_state=1)\n",
    "      X_train, X_test, y_train, y_test = train_test_split(x_train, y, test_size=0.2, random_state= i+1)\n",
    "      adbclassifier.fit(X_train, y_train)\n",
    "      scores_ada.append(adbclassifier.score(X_test,y_test))\n",
    "      trainscores_ada.append(adbclassifier.score(X_train,y_train))\n",
    "print(np.mean(scores_ada))\n",
    "print(len(X_test))\n",
    "plt.ylim(0,1)\n",
    "plt.plot(scores_ada)\n",
    "plt.plot(trainscores_ada)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_func(X_train, X_test, y_train, y_test):\n",
    "    ac_xgb1 = []\n",
    "    ac_xgb2 = []\n",
    "\n",
    "    dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "    dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "\n",
    "    # Define hyperparameters\n",
    "        params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}\n",
    "\n",
    "        n = 100\n",
    "        xgb_model1 = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain_reg,\n",
    "        num_boost_round=n,\n",
    "        )\n",
    "        evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]\n",
    "        xgb_model2 = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain_reg,\n",
    "        num_boost_round=n,\n",
    "        evals=evals,\n",
    "        verbose_eval=250,\n",
    "        )\n",
    "        preds = xgb_model1.predict(dtest_reg)\n",
    "        preds1 = xgb_model2.predict(dtest_reg)\n",
    "        preds = np.round(preds)\n",
    "        preds1 = np.round(preds1)\n",
    "        error= np.sum(preds!=y_test)\n",
    "    # print(preds!=y_test)\n",
    "        accuracy = 1 - error/preds.size\n",
    "        ac_xgb1.append(accuracy)\n",
    "        error1 = np.sum(preds1 != y_test)\n",
    "        accuracy2 = 1- error1/preds1.size\n",
    "        ac_xgb2.append(accuracy2)\n",
    "    print(np.mean(ac_xgb1))\n",
    "    print(np.mean(ac_xgb2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic_func(X_train, X_test, y_train, y_test)\n",
    "# svm_func(X_train, X_test, y_train, y_test)\n",
    "# randomforest_func(X_train, X_test, y_train, y_test)\n",
    "# neuralnetworks_func(X_train, X_test, y_train, y_test)\n",
    "# adaboost_func(X_train, X_test, y_train, y_test)\n",
    "# xgboost_func(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which gives predicted and correct values with missclassifications and scores for rest models\n",
    "def model_predict(X_test, y_test1, model):\n",
    "    predictions = model.predict(X_test)\n",
    "    #y_test1 = y_test1.astype('int64')\n",
    "    #testYarg = np.argmax(y_test1, axis = 1)\n",
    "    miss_class = np.where(predictions != y_test1)\n",
    "    miss_class = miss_class[0]\n",
    "    score=model.score(X_test, y_test1)\n",
    "    # print(model)\n",
    "    # print(\"Predicted:\",predictions)\n",
    "    # print(\"Correct:  \",testYarg)\n",
    "    # print(\"Index missclassified:\", miss_class)\n",
    "    # print(\"Score:\", score, \"\\n\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict_neural(X_test, y_test1, model):\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = np.argmax(predictions, axis = 1)\n",
    "    y_test1 = y_test1.astype('int64')\n",
    "    testYarg = np.argmax(y_test1, axis = 1)\n",
    "    miss_class = np.where(predictions != testYarg)\n",
    "    miss_class = miss_class[0]\n",
    "    print(\"Neural network\")\n",
    "    print(\"Predicted:\",predictions)\n",
    "    print(\"Correct:  \",testYarg)\n",
    "    print(\"Index missclassified:\", miss_class)\n",
    "    # print(\"Score:\", val_accuracy, \"\\n\")\n",
    "    return predictions, miss_class, testYarg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'S3'\n",
    "X_test = total_features[total_features['index'].str.startswith(subject)]\n",
    "y_test = X_test['y']\n",
    "X_test = X_test.drop(['y', 'index'], axis=1)\n",
    "X_train = total_features[total_features['index'].str.startswith(subject)]\n",
    "y_train = X_train['y']\n",
    "X_train = X_train.drop(['y', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cat = to_categorical(y_test)\n",
    "y_test1 = y_test_cat.astype('int64')\n",
    "testYarg = np.argmax(y_test1, axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S13','S14','S15','S16','S17']\n",
    "model_names = [\"LogisticRegression(max_iter=3000)\", \"SVC()\", \"RandomForestClassifier()\", \"AdaBoostClassifier(learning_rate=1, random_state=5)\"]\n",
    "\n",
    "feat_dict = dict()\n",
    "scores_dict = dict()\n",
    "for model in model_names:\n",
    "    scores_dict[model] = []\n",
    "\n",
    "for i in range(len(selected_features)):\n",
    "    feat = selected_features[i]\n",
    "    if \"PPG_Rate_Max\" in feat:\n",
    "         feat.remove(\"PPG_Rate_Max\")\n",
    "\n",
    "    for subject in subjects:\n",
    "        X_test = total_features[total_features['index'].str.startswith(subject)]\n",
    "        y_test = X_test['y']\n",
    "        X_test = X_test.drop(['y', 'index'], axis=1)\n",
    "        X_train = total_features[~total_features['index'].str.startswith(subject)]\n",
    "        y_train = X_train['y']\n",
    "        X_train = X_train.drop(['y', 'index'], axis=1)\n",
    "        X_train = X_train[list(feat)]\n",
    "        X_test = X_test[list(feat)]\n",
    "        \n",
    "        # fit the data\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Get models\n",
    "        lg_model = logistic_func(X_train, X_test, y_train, y_test)\n",
    "        svm_model = svm_func(X_train, X_test, y_train, y_test)\n",
    "        rf_model = randomforest_func(X_train, X_test, y_train, y_test)\n",
    "        # nn_model = neuralnetworks_func(X_train, X_test, Y_train, Y_test)\n",
    "        ada_model = adaboost_func(X_train, X_test, Y_train, Y_test)\n",
    "        # Get the score\n",
    "        models= [lg_model, svm_model, rf_model, ada_model]\n",
    "\n",
    "        for model in models:\n",
    "            scores_dict[str(model)].append(model_predict(X_test, y_test, model))\n",
    "\n",
    "        # scores_dict[str(nn_model)].append(model_predict_neural(X_test, y_test, nn_model))\n",
    "    feat_dict[feature_sel_name[i]] = scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLogisticRegression(max_iter=3000)\tSVC()\tRandomForestClassifier()\tAdaBoostClassifier(learning_rate=1, random_state=5)\n",
      "variance threshold\t\t0.873+-0.117\t0.873+-0.106\t0.860+-0.113\t0.841+-0.130\n",
      "pearson correlation\t\t0.873+-0.117\t0.873+-0.106\t0.860+-0.113\t0.841+-0.130\n",
      "chi-squared\t\t0.873+-0.117\t0.873+-0.106\t0.860+-0.113\t0.841+-0.130\n",
      "forward wrapper\t\t0.873+-0.117\t0.873+-0.106\t0.860+-0.113\t0.841+-0.130\n",
      "backwards wrapper\t\t0.873+-0.117\t0.873+-0.106\t0.860+-0.113\t0.841+-0.130\n",
      "step-wise wrapper\t\t0.873+-0.117\t0.873+-0.106\t0.860+-0.113\t0.841+-0.130\n",
      "Lasso\t\t0.873+-0.117\t0.873+-0.106\t0.860+-0.113\t0.841+-0.130\n",
      "Random Forest\t\t0.873+-0.117\t0.873+-0.106\t0.860+-0.113\t0.841+-0.130\n",
      "lightGBM\t\t0.873+-0.117\t0.873+-0.106\t0.860+-0.113\t0.841+-0.130\n"
     ]
    }
   ],
   "source": [
    "first_line = \"\"\n",
    "for model in models:\n",
    "    first_line += \"\\t\" +str(model)\n",
    "print(first_line)\n",
    "\n",
    "for feat in feat_dict:\n",
    "    line = str(feat) + \"\\t\"\n",
    "\n",
    "    for model in feat_dict[feat]:\n",
    "        line += \"\\t\" + f\"{np.mean(scores_dict[str(model)]):.3f}\" + \"+-\" + f\"{np.std(scores_dict[str(model)]):.3f}\"\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
